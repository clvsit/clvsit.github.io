<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>论文阅读：Enhancing Chat Language Models by Scaling High-quality Instructional Conversations 构造多轮指令对话数据集 | clvsit 个人博客</title><meta name="author" content="clvsit"><meta name="copyright" content="clvsit"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="数据集地址：https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;stingning&#x2F;ultrachatGitHub 仓库：https:&#x2F;&#x2F;github.com&#x2F;thunlp&#x2F;UltraChat 对指令数据进行微调已被广泛认为是实施 ChatGPT 等聊天语言模型的有效方法。本文旨在通过构建一个系统设计的、多样化的、信息丰富的大规模指令对话数据集 UltraChat，来提高开源模型性能。">
<meta property="og:type" content="article">
<meta property="og:title" content="论文阅读：Enhancing Chat Language Models by Scaling High-quality Instructional Conversations 构造多轮指令对话数据集">
<meta property="og:url" content="https://clvsit.github.io/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AEnhancing-Chat-Language-Models-by-Scaling-High-quality-Instructional-Conversations-%E6%9E%84%E9%80%A0%E5%A4%9A%E8%BD%AE%E6%8C%87%E4%BB%A4%E5%AF%B9%E8%AF%9D%E6%95%B0%E6%8D%AE%E9%9B%86/">
<meta property="og:site_name" content="clvsit 个人博客">
<meta property="og:description" content="数据集地址：https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;stingning&#x2F;ultrachatGitHub 仓库：https:&#x2F;&#x2F;github.com&#x2F;thunlp&#x2F;UltraChat 对指令数据进行微调已被广泛认为是实施 ChatGPT 等聊天语言模型的有效方法。本文旨在通过构建一个系统设计的、多样化的、信息丰富的大规模指令对话数据集 UltraChat，来提高开源模型性能。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Enhancing%20Chat%20Language%20Models%20by%20Scaling%20High-quality%20Instructional%20Conversations/Table%207.png">
<meta property="article:published_time" content="2023-09-03T15:09:59.000Z">
<meta property="article:modified_time" content="2024-06-21T15:37:39.258Z">
<meta property="article:author" content="clvsit">
<meta property="article:tag" content="论文阅读">
<meta property="article:tag" content="数据构造">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Enhancing%20Chat%20Language%20Models%20by%20Scaling%20High-quality%20Instructional%20Conversations/Table%207.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://clvsit.github.io/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AEnhancing-Chat-Language-Models-by-Scaling-High-quality-Instructional-Conversations-%E6%9E%84%E9%80%A0%E5%A4%9A%E8%BD%AE%E6%8C%87%E4%BB%A4%E5%AF%B9%E8%AF%9D%E6%95%B0%E6%8D%AE%E9%9B%86/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '论文阅读：Enhancing Chat Language Models by Scaling High-quality Instructional Conversations 构造多轮指令对话数据集',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-06-21 23:37:39'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.1.1"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">81</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">64</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Enhancing%20Chat%20Language%20Models%20by%20Scaling%20High-quality%20Instructional%20Conversations/Table%207.png')"><nav id="nav"><span id="blog-info"><a href="/" title="clvsit 个人博客"><span class="site-name">clvsit 个人博客</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">论文阅读：Enhancing Chat Language Models by Scaling High-quality Instructional Conversations 构造多轮指令对话数据集</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-09-03T15:09:59.000Z" title="发表于 2023-09-03 23:09:59">2023-09-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-06-21T15:37:39.258Z" title="更新于 2024-06-21 23:37:39">2024-06-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/LLM/">LLM</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/LLM/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/">数据增强</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/LLM/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/%E6%95%B0%E6%8D%AE%E6%9E%84%E9%80%A0/">数据构造</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>11分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="论文阅读：Enhancing Chat Language Models by Scaling High-quality Instructional Conversations 构造多轮指令对话数据集"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>数据集地址：<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/stingning/ultrachat">https://huggingface.co/datasets/stingning/ultrachat</a><br>GitHub 仓库：<a target="_blank" rel="noopener" href="https://github.com/thunlp/UltraChat">https://github.com/thunlp/UltraChat</a></p>
<p>对指令数据进行微调已被广泛认为是实施 ChatGPT 等聊天语言模型的有效方法。本文旨在通过构建一个系统设计的、多样化的、信息丰富的大规模指令对话数据集 UltraChat，来提高开源模型性能。UltraChat 包含 150 万条高质量的多轮对话，涵盖广泛的主题和指令。作者在 UltraChat 的基础上，对 LLaMA 模型进行了微调，从而创建了强大的会话模型 UltraLLaMA。评估结果表明，UltraLLaMA 始终优于其他开源模型，包括之前公认的最先进开源模型 Vicuna。</p>
<h1 id="方法介绍"><a href="#方法介绍" class="headerlink" title="方法介绍"></a>方法介绍</h1><p>与其他倾向于使用特定任务（如问题解答、改写和总结）来构建数据的数据集不同，该方法以一个三方框架为基础，旨在捕捉人类与人工智能助手可能进行的广泛交互。作者认为，人类用户与人工智能助手之间的任何互动都可视为获取信息。</p>
<ul>
<li><strong>信息获取</strong>：第一部分“关于世界的问题”侧重于查询世界上的现有信息。这是人机交互的一个重要方面，因为用户通常依赖于人工智能助手来快速准确地回答他们的问题。通过包含广泛的主题，数据集满足了用户对信息的不同需求，确保人工智能助手能够提供相关和全面的回复。</li>
<li><strong>创建条件信息</strong>：第二部分是“创作与写作”，涉及在人类输入条件下创造新信息。这一过程反映了人工智能与用户一起参与创造性任务的能力，利用其丰富的知识和模式识别能力生成原创内容。</li>
<li><strong>信息改造</strong>：第三部分“对现有材料的协助”涉及对现有信息的修改。这是人机交互的一个重要方面，因为它可以让人工智能助手主动参与用户的输入，通过改写、续写、总结或推理等各种方式对其进行转换。</li>
</ul>
<p>如上所述，UltraChat 由三个不同的部分组成，每个部分都面临着独特的挑战。首要原则是使数据尽可能多样化。确保数据多样性的核心是确保开场白和用户回复风格的多样性。</p>
<ul>
<li>开场白直接决定了对话的主题。开场白应高度多样化，包含人类用户可能要求聊天模型执行的任何任务。</li>
<li>用户决定对话的情节，输出应根据当前主题量身定制，语言风格和要求应多样化。</li>
</ul>
<h2 id="第一部分：关于世界的问题"><a href="#第一部分：关于世界的问题" class="headerlink" title="第一部分：关于世界的问题"></a>第一部分：关于世界的问题</h2><p>主要关注现实世界中存在的概念、对象和实体。收集数据的方法涉及两个视角：</p>
<ul>
<li><p>以主题和概念为中心：</p>
<ol>
<li>最初，要求 ChatGPT 生成 30 个综合话题，这些话题涵盖了日常生活的各个方面。</li>
<li>随后，深入研究每个话题，生成 30 到 50 个子话题或相关概念。</li>
<li>最后，为每个子话题或概念生成 10 个不同的问题，并要求 ChatGPT 在每个原始问题的基础上再生成 10 个问题。</li>
</ol>
</li>
<li><p>以现实世界中的实体为中心：这些对象来自维基数据实体。考虑到这些实体在维基百科文章中出现的频率，对它们进行了进一步的细化，尤其是重点关注出现频率最高的 10,000 个实体。对于每个实体，创建 5 个元问题，然后是 10 个更具体的问题和 20 个扩展问题。扩展问题旨在与原始问题保持一定的相似性，同时探索不同的对象或主题。为了创建对话，过滤并抽取了大约 50 万个问题作为开场白。在构建每段对话的过程中，都会为用户模型提供精心制作的提示，明确要求模型根据正在进行的对话历史背景做出简洁而有意义的回应。</p>
</li>
</ul>
<h2 id="第二部分：创造和生成"><a href="#第二部分：创造和生成" class="headerlink" title="第二部分：创造和生成"></a>第二部分：创造和生成</h2><p>为了创建对话，作者过滤并抽取了大约 50 万个问题作为开场白。在构建每段对话的过程中，都会为用户模型提供精心制作的 prompt，明确要求模型根据正在进行的对话历史背景做出简洁而有意义的回应。这些指令是对话生成的开场白。在整个生成过程中，用户 prompt 会不断强化对话的主要目的，即生成和完善一篇文章。这样做的目的是确保用户模型的行为与预期目的保持一致。</p>
<h2 id="第三部分：为现有材料提供协助"><a href="#第三部分：为现有材料提供协助" class="headerlink" title="第三部分：为现有材料提供协助"></a>第三部分：为现有材料提供协助</h2><p>与现有文本材料相关的各种任务，如改写、翻译、摘要和问题解答等。首先从 C4 语料库中收集文本片段。C4 语料库中的每篇文章都与源 URL 相关联。为确保文本内容和风格的多样性，采用了上一节中列出的 20 种材料类型，并为每种类型手动设置了关键词。此外，还通过匹配关键词和相应的 URL 对语料库中的文本进行分类。总共从 C4 语料库中收集了 10,000 篇文本，并针对每篇文本提示 ChatGPT 生成五条不同的指令。为了将文本片段与特定指令结合起来，作者使用了人工设计的模板，如表 4 所示。最终，500,000 个片段的组合将作为生成对话的开场白。</p>
<p><img src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Enhancing%20Chat%20Language%20Models%20by%20Scaling%20High-quality%20Instructional%20Conversations/Table%204.png"></p>
<blockquote>
<p>表 4：手工设计的模板，用于连接现有材料和生成的指令。</p>
</blockquote>
<h2 id="用户模拟和强化"><a href="#用户模拟和强化" class="headerlink" title="用户模拟和强化"></a>用户模拟和强化</h2><p>保持用户模型的理想行为是成功实现自动对话生成的关键。据观察，当用户模型只获得当前对话历史记录时，它往往会扮演人工智能助手的角色。这种“角色互换”的情况会严重影响多轮对话的连贯性。为了解决这个问题，除了展示对话历史之外，作者还加入了一些 prompt，明确指示模型采用不同的用户性格。在第二部分中，采用了一个 prompt 来提醒模型对话的主要目的，从而促进对话更加自然流畅。数据生成过程完成后，会进一步过滤，以确保整体数据质量。为了增强用户回答的真实性，特别排除了“Thank you”、“Thanks”和“You’re welcome”等过于礼貌的语句。</p>
<h2 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h2><p>作者对 UltraChat 和其他几个教学数据集进行了统计分析，如表 5 所示。UltraChat 在规模上非常突出，是最大的公开可用数据集之一。此外，它还显示了最高的平均回合数和最长的每个数据实例的平均长度。</p>
<p><img src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Enhancing%20Chat%20Language%20Models%20by%20Scaling%20High-quality%20Instructional%20Conversations/Table%205.png"></p>
<blockquote>
<p>表 5：现有教学数据集的统计数据。词法多样性是通过平均每个语篇的 MTLD 分数（McCarthy 和 Jarvis，2010 年）计算出来的。从每个数据集中随机抽取 10000 个样本，用于测量主题多样性和一致性。话题多样性是通过使用 OpenAI embedding API 平均每对数据之间的余弦距离来测量的。连贯性由 ChatGPT 以 1-10 分来评分。</p>
</blockquote>
<p>为了评估多样性，作者测量了词汇多样性和主题多样性。在词汇多样性方面，UltraChat 优于之前的数据集。然而，在话题多样性方面，UltraChat 与 GPT4ALL 相比略有不足，但仍大大超过了其他数据集。这可能是由于每个数据实例中的 token 数量相对较多，从而规范了每个对话的数据嵌入（GPT4ALL 数据集在单轮中）。为了确保多轮对话的一致性，作者还进行了一致性评估。结果表明，大多数数据集都表现出了相对较高的一致性。值得注意的是，UltraChat 和 Baize 数据的一致性排名最高。</p>
<h2 id="基于-UltraChat-数据集训练-UltraLLaMA"><a href="#基于-UltraChat-数据集训练-UltraLLaMA" class="headerlink" title="基于 UltraChat 数据集训练 UltraLLaMA"></a>基于 UltraChat 数据集训练 UltraLLaMA</h2><p>作者在 UltraChat 数据集上对 LLaMA-13B 模型进行了训练，从而开发出了 LLaMA-13B 的增强变体 UltraLLaMA。为了提高模型对对话上下文的理解能力，将每段对话分解成更小的序列，限制它们的最大长度为 2048 个 tokens。在训练过程中，只计算模型回答的 loss。这种方法确保了模型能够从对话的早期部分获取相关信息，从而能够更全面地理解正在进行的对话。通过结合之前的上下文，UltraLLaMA 能够生成更符合上下文、更连贯的回答。使用标准的交叉熵损失对模型进行微调。该模型使用 128 个 A100 GPU 进行训练，总 batch_size 为 512。</p>
<h1 id="结果评估"><a href="#结果评估" class="headerlink" title="结果评估"></a>结果评估</h1><p>评估聊天模型生成的回复质量是一项重大挑战，尤其是考虑到不同环境下可能存在的不稳定性。传统的基准一直被用于评估目的；但是，当代的方法是利用 ChatGPT 和 GPT-4 等高级模型。在作者的初步实验中，与人工评估相比，这种做法已被证明能产生更可靠的结果。</p>
<p><strong>验证集</strong>：这套评估包括 Vicuna 基准以及由 GPT-4 生成的另外 300 个问题和说明。这些问题&#x2F;指令涵盖了广泛的主题，包括常识、世界知识、专业知识（特别是物理和生物）、数学、反应生成和写作任务。此外，问题集的每个部分还根据不同的难度进一步分类。表 6 列出了评估问题集的一些示例。</p>
<p><img src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Enhancing%20Chat%20Language%20Models%20by%20Scaling%20High-quality%20Instructional%20Conversations/Figure%206.png"></p>
<blockquote>
<p>表 6：作者创建的验证集的一些示例。</p>
</blockquote>
<p><strong>基线模型</strong>：作者选用 Alpaca、Vicuna-13B、Koala-13B、Dolly-V2 和 OpenAssistant-12B 作为基线模型。</p>
<h2 id="输出比较"><a href="#输出比较" class="headerlink" title="输出比较"></a>输出比较</h2><p>使用 ChatGPT 将模型输出与每个基线模型在每个问题上的输出进行比较。具体来说，分别从两个模型中输入问题和一对独立的答案，并让 ChatGPT 对每个答案进行 1 到 10 分的评分，并提供给定分数的理由。评估 prompt 旨在优先考虑正确性，而不是信息量等其他因素。</p>
<p><strong>回答的顺序会对评估结果产生重大影响。为了解决这个问题，随机确定了每个问题的回答顺序</strong>。最后，统计了每个基线模型的胜&#x2F;平&#x2F;负次数，结果如图 2 所示。</p>
<p><img src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Enhancing%20Chat%20Language%20Models%20by%20Scaling%20High-quality%20Instructional%20Conversations/Figure%202.png"></p>
<blockquote>
<p>图 2：UltraLLaMA 与其他基线在策划评估集上的输出比较。评估由 ChatGPT 完成。</p>
</blockquote>
<p>可以看出，与评估集中的所有开源模型相比，UltraLLaMA 都表现出了卓越的性能，胜率高达 85%，令人印象深刻。值得注意的是，UltraLLaMA 的胜率还比 Vicuna 高出 13%。</p>
<h2 id="独立评分"><a href="#独立评分" class="headerlink" title="独立评分"></a>独立评分</h2><p>考虑到成对比较的不稳定性，作者还通过 ChatGPT 进行了独立评分，根据回答质量从 1 到 10 分不等。表 7 展示了 UltraLLaMA 与基准模型之间的评分比较。</p>
<p><img src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Enhancing%20Chat%20Language%20Models%20by%20Scaling%20High-quality%20Instructional%20Conversations/Table%207.png"></p>
<blockquote>
<p>表 7：每个模型在经过策划的评估集上的总体得分和分段得分。得分介于 1 到 10 之间，报告的是平均得分。粗体表示最佳得分，下划线表示次佳得分。</p>
</blockquote>
<p>值得注意的是，与所有开源模型相比，UltraLLaMA 在总分上表现出了显著的优势。此外，UltraLLaMA 几乎在评估集的每个部分都取得了最高的性能，展示了其非凡的能力。</p>
<p>这种细分还有助于深入了解每种模型在特定类型的问题和指令上的表现。一般来说，<strong>所有模型在与常识性知识和对世界的一般理解有关的较简单问题上的表现都较好</strong>。然而，事实证明，<strong>涉及推理和创造性写作的较复杂任务对大多数模型来说都具有挑战性</strong>。有趣的是，尽管 Alpaca 只有 70 亿个参数，但在与常识和世界知识相关的问题上，它的表现却比大型模型要好。然而，在要求更高的任务上，它就落后了。此外，值得注意的是，基于 Pythia（Biderman 等人，2023 年）的 Dolly 和 OpenAssistant 与基于 LLaMA 的类似甚至更小的模型相比，显示出更差的性能。这一观察结果凸显了<strong>底层骨干语言模型的重要性</strong>。</p>
<h2 id="系统-prompt-的影响"><a href="#系统-prompt-的影响" class="headerlink" title="系统 prompt 的影响"></a>系统 prompt 的影响</h2><p>使用系统 prompt 来提示 LLM 的角色和输出风格是一种常见的做法。在作者的评估中，发现<strong>系统 prompt 对生成输出的风格有很大影响</strong>。具体来说，当系统提示模型提供“有帮助的详细”回复时，模型往往会生成更多相关细节，从而提高回复的信息量。虽然这些 prompt 可能不会直接影响确定性问题的准确性，但它们确实会影响额外信息的提供，从而进一步提高答复的整体质量。</p>
<p><img src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Enhancing%20Chat%20Language%20Models%20by%20Scaling%20High-quality%20Instructional%20Conversations/Table%209.png"></p>
<blockquote>
<p>表 9：有系统 prompt 和无系统 prompt 的 UltraLLaMA 比较。</p>
</blockquote>
<p>为了说明这种影响，可以参考表 9，在该表中，两个输出都是正确的，但在系统 prompt 指导下的模型得到的回答信息量更大。</p>
<blockquote>
<p>PS：2023 年 2 月份的一篇论文《Guidling Large Language Models via Directional Stimulus Prompting》提出了一个名为“定向刺激 prompt”（Directional Stimulus Prompting）的新型 prompt 框架，先训练一个经过微调、强化学习后的小型模型，然后使用该小型模型根据用户的查询来生成对应的刺激（文本），将其添加到 prompt 中来引导黑盒大语言模型朝着所需的输出方向前进。我理解更改系统 prompt 和情感刺激的作用是类似的，引导模型输出向着自己想要的方向发展。</p>
</blockquote>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本篇工作介绍了 <strong>UltraChat，这是一种结构化设计的多轮指令对话数据</strong>，对聊天语言模型进行了有效的微调，显著提升了模型性能。UltraChat 的设计理念和方法为未来聊天模型的发展提供了新的方向，证明了高质量指令对话数据集在提升模型性能方面的重要性。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://clvsit.github.io">clvsit</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://clvsit.github.io/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AEnhancing-Chat-Language-Models-by-Scaling-High-quality-Instructional-Conversations-%E6%9E%84%E9%80%A0%E5%A4%9A%E8%BD%AE%E6%8C%87%E4%BB%A4%E5%AF%B9%E8%AF%9D%E6%95%B0%E6%8D%AE%E9%9B%86/">https://clvsit.github.io/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AEnhancing-Chat-Language-Models-by-Scaling-High-quality-Instructional-Conversations-%E6%9E%84%E9%80%A0%E5%A4%9A%E8%BD%AE%E6%8C%87%E4%BB%A4%E5%AF%B9%E8%AF%9D%E6%95%B0%E6%8D%AE%E9%9B%86/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://clvsit.github.io" target="_blank">clvsit 个人博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a><a class="post-meta__tags" href="/tags/%E6%95%B0%E6%8D%AE%E6%9E%84%E9%80%A0/">数据构造</a></div><div class="post_share"><div class="social-share" data-image="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Enhancing%20Chat%20Language%20Models%20by%20Scaling%20High-quality%20Instructional%20Conversations/Table%207.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/LLM-%E6%8E%A8%E7%90%86%E7%9B%B8%E5%85%B3/" title="LLM - 推理相关资料整理"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">LLM - 推理相关资料整理</div></div></a></div><div class="next-post pull-right"><a href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ASelf-Consistency-Improves-Chain-of-Thought-Reasoning-in-Language-Models/" title="论文阅读：Self-Consistency Improves Chain of Thought Reasoning in Language Models"><img class="cover" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Self-Consistency%20Improves%20Chain%20of%20Thought%20Reasoning%20in%20Language%20Models/Figure%201.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">论文阅读：Self-Consistency Improves Chain of Thought Reasoning in Language Models</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/%E5%B1%82%E5%89%AA%E6%9E%9D%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AB%81%E6%8E%A5%E7%9A%84%E2%80%9C%E5%8F%8C%E7%94%9F%E8%8A%B1%E2%80%9D/" title="论文阅读：The Unreasonable Ineffectiveness of the Deeper Layers 层剪枝与模型嫁接的“双生花”"><img class="cover" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/The%20Unreasonable%20Ineffectiveness%20of%20the%20Deeper%20Layers/Figure%201.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-10</div><div class="title">论文阅读：The Unreasonable Ineffectiveness of the Deeper Layers 层剪枝与模型嫁接的“双生花”</div></div></a></div><div><a href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AAlbert%EF%BC%9AA-Lite-Bert-for-self-supervised-learning-of-language-representations/" title="论文阅读：Albert：A Lite Bert for self-supervised learning of language representations"><img class="cover" src="https://pic1.zhimg.com/80/v2-8ea109ed6f3a3781951402a6cf7cc586_720w.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-28</div><div class="title">论文阅读：Albert：A Lite Bert for self-supervised learning of language representations</div></div></a></div><div><a href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ABag-of-Tricks-for-Efficient-Text-Classification/" title="论文阅读：Bag of Tricks for Efficient Text Classification"><img class="cover" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Bag%20of%20Tricks%20for%20Efficient%20Text%20Classification/FIgure%201.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-01-22</div><div class="title">论文阅读：Bag of Tricks for Efficient Text Classification</div></div></a></div><div><a href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADeeBERT%EF%BC%9ADynamic-Early-Exiting-for-Accelerating-BERT-Inference/" title="论文阅读：DeeBERT：Dynamic Early Exiting for Accelerating BERT Inference"><img class="cover" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/DeeBERT%20Dynamic%20Early%20Exiting%20for%20Accelerating%20BERT%20Inference/Figure%201.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-17</div><div class="title">论文阅读：DeeBERT：Dynamic Early Exiting for Accelerating BERT Inference</div></div></a></div><div><a href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADistilBERT-a-distilled-vesion-of-BERT-smaller-faster-cheaper-and-lighter/" title="论文阅读：DistilBERT a distilled vesion of BERT smaller faster cheaper and lighter"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-14</div><div class="title">论文阅读：DistilBERT a distilled vesion of BERT smaller faster cheaper and lighter</div></div></a></div><div><a href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AEDA%EF%BC%9AEasy-Data-Augmentation-Techniques-for-Boosting-Performance-on-Text-Classification-Tasks/" title="论文阅读：EDA：Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks"><img class="cover" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/EDA%20Easy%20Data%20Augmentation%20Techniques%20for%20Boosting%20Performance%20on%20Text%20Classification%20Tasks/EDA%20Figure1.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-03-03</div><div class="title">论文阅读：EDA：Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">clvsit</div><div class="author-info__description">人生不是戏剧，而我亦非主角</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">81</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">64</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/clvsit"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/clvsit" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">知乎和 CSDN 同名 clvsit，目前在将本地的笔记逐步迁移到这，所以会看到过去日期的文章不断增多</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.</span> <span class="toc-text">方法介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%EF%BC%9A%E5%85%B3%E4%BA%8E%E4%B8%96%E7%95%8C%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">1.1.</span> <span class="toc-text">第一部分：关于世界的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%EF%BC%9A%E5%88%9B%E9%80%A0%E5%92%8C%E7%94%9F%E6%88%90"><span class="toc-number">1.2.</span> <span class="toc-text">第二部分：创造和生成</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%EF%BC%9A%E4%B8%BA%E7%8E%B0%E6%9C%89%E6%9D%90%E6%96%99%E6%8F%90%E4%BE%9B%E5%8D%8F%E5%8A%A9"><span class="toc-number">1.3.</span> <span class="toc-text">第三部分：为现有材料提供协助</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%A8%E6%88%B7%E6%A8%A1%E6%8B%9F%E5%92%8C%E5%BC%BA%E5%8C%96"><span class="toc-number">1.4.</span> <span class="toc-text">用户模拟和强化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90"><span class="toc-number">1.5.</span> <span class="toc-text">数据分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E-UltraChat-%E6%95%B0%E6%8D%AE%E9%9B%86%E8%AE%AD%E7%BB%83-UltraLLaMA"><span class="toc-number">1.6.</span> <span class="toc-text">基于 UltraChat 数据集训练 UltraLLaMA</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C%E8%AF%84%E4%BC%B0"><span class="toc-number">2.</span> <span class="toc-text">结果评估</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E6%AF%94%E8%BE%83"><span class="toc-number">2.1.</span> <span class="toc-text">输出比较</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8B%AC%E7%AB%8B%E8%AF%84%E5%88%86"><span class="toc-number">2.2.</span> <span class="toc-text">独立评分</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%B3%BB%E7%BB%9F-prompt-%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">2.3.</span> <span class="toc-text">系统 prompt 的影响</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">3.</span> <span class="toc-text">总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/AI-%E6%83%85%E6%84%9F%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B9%8B%E6%97%85%E2%80%94%E2%80%94%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E6%94%B6%E9%9B%86/" title="AI-情感聊天机器人之旅——相关论文收集">AI-情感聊天机器人之旅——相关论文收集</a><time datetime="2024-06-20T13:15:26.000Z" title="发表于 2024-06-20 21:15:26">2024-06-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/PyramidKV/" title="PyramidKV">PyramidKV</a><time datetime="2024-06-14T12:27:21.000Z" title="发表于 2024-06-14 20:27:21">2024-06-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/Long-Context-%E8%B0%83%E7%A0%94/" title="Long Context 调研">Long Context 调研</a><time datetime="2024-06-10T14:43:57.000Z" title="发表于 2024-06-10 22:43:57">2024-06-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/vocab-size-%E7%A0%94%E7%A9%B6/" title="vocab size 调研">vocab size 调研</a><time datetime="2024-05-31T15:47:07.000Z" title="发表于 2024-05-31 23:47:07">2024-05-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARCoT-Detecting-and-Rectifying-Factual-Inconsistency-in-Reasoning-by-Reversing-Chain-of-Thought/" title="论文阅读：RCoT Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought"><img src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/RCoT%EF%BC%9ADetecting%20and%20Rectifying%20Factual%20Inconsistency%20in%20Reasoning%20by%20Reversing%20Chain-of-Thought/Figure%204.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读：RCoT Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought"/></a><div class="content"><a class="title" href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARCoT-Detecting-and-Rectifying-Factual-Inconsistency-in-Reasoning-by-Reversing-Chain-of-Thought/" title="论文阅读：RCoT Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought">论文阅读：RCoT Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought</a><time datetime="2024-05-17T13:37:04.000Z" title="发表于 2024-05-17 21:37:04">2024-05-17</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Enhancing%20Chat%20Language%20Models%20by%20Scaling%20High-quality%20Instructional%20Conversations/Table%207.png')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By clvsit</div><div class="footer_custom_text">城南芳草城北溪，池塘烟柳归鸟栖。忽听窗外风吹雨，一梦花落月近西。</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"]):not([href="/music/"]):not([href="/no-pjax/"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener
  btf.removeGlobalFnEvent('pjax')
  btf.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>