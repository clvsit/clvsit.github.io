<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>clvsit 个人博客</title><meta name="author" content="clvsit"><meta name="copyright" content="clvsit"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="人生不是戏剧，而我亦非主角">
<meta property="og:type" content="website">
<meta property="og:title" content="clvsit 个人博客">
<meta property="og:url" content="https://clvsit.github.io/page/5/">
<meta property="og:site_name" content="clvsit 个人博客">
<meta property="og:description" content="人生不是戏剧，而我亦非主角">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://clvsit.github.io/img/avatar.jpg">
<meta property="article:author" content="clvsit">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://clvsit.github.io/img/avatar.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://clvsit.github.io/page/5/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'clvsit 个人博客',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2024-07-27 20:52:25'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.1.1"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">83</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">67</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 专题</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/prompt-engineer"><i class="fa-fw fas fa-archive"></i><span> Prompt 工程</span></a></li><li><a class="site-page child" href="/RAG"><i class="fa-fw fas fa-archive"></i><span> RAG</span></a></li><li><a class="site-page child" href="/long-context"><i class="fa-fw fas fa-archive"></i><span> Long Context</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> LLM</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/LLM-%E5%B9%BB%E8%A7%89%E9%97%AE%E9%A2%98"><i class="fa-fw fas fa-archive"></i><span> 幻觉问题</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image: url('/img/banner.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="clvsit 个人博客"><span class="site-name">clvsit 个人博客</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 专题</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/prompt-engineer"><i class="fa-fw fas fa-archive"></i><span> Prompt 工程</span></a></li><li><a class="site-page child" href="/RAG"><i class="fa-fw fas fa-archive"></i><span> RAG</span></a></li><li><a class="site-page child" href="/long-context"><i class="fa-fw fas fa-archive"></i><span> Long Context</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> LLM</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/LLM-%E5%B9%BB%E8%A7%89%E9%97%AE%E9%A2%98"><i class="fa-fw fas fa-archive"></i><span> 幻觉问题</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">clvsit 个人博客</h1><div id="site-subtitle"><span id="subtitle"></span></div><div id="site_social_icons"><a class="social-icon" href="https://github.com/clvsit" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left"><a href="/DFA-%E7%AE%97%E6%B3%95/" title="DFA 算法"><img class="post-bg" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9zczAuYmRzdGF0aWMuY29tLzcwY0Z2SFNoX1ExWW54R2twb1dLMUhGNmhoeS9pdC91PTI4MjkyNDkzMzQsOTQ1MjgzNTMyJmZtPTExJmdwPTAuanBn?x-oss-process=image/format,png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="DFA 算法"></a></div><div class="recent-post-info"><a class="article-title" href="/DFA-%E7%AE%97%E6%B3%95/" title="DFA 算法">DFA 算法</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2020-05-03T14:43:02.000Z" title="发表于 2020-05-03 22:43:02">2020-05-03</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a></span></div><div class="content">DFA，全称 Deterministic Finite Automaton 即确定有穷自动机：从一个状态通过一系列的事件转换到另一个状态，即 state -&gt; event -&gt; state。

确定：状态以及引起状态转换的事件都是可确定的，不存在“意外”。
有穷：状态以及事件的数量都是可穷举的。

计算机操作系统中的进程状态与切换可以作为 DFA 算法的一种近似理解。如下图所示，其中椭圆表示状态，状态之间的连线表示事件，进程的状态以及事件都是可确定的，且都可以穷举。

DFA 算法具有多种应用，在此先介绍在匹配关键词领域的应用。
匹配关键词我们可以将每个文本片段作为状态，例如“匹配关键词”可拆分为“匹”、“匹配”、“匹配关”、“匹配关键”和“匹配关键词”五个文本片段。

【过程】：

初始状态为空，当触发事件“匹”时转换到状态“匹”；
触发事件“配”，转换到状态“匹配”；
依次类推，直到转换为最后一个状态“匹配关键词”。

再让我们考虑多个关键词的情况，例如“匹配算法”、“匹配关键词”以及“信息抽取”。

可以看到上图的状态图类似树形结构，也正是因为这个结构，使得 DFA  ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/gitlab%E5%88%9B%E5%BB%BARepoPage/" title="gitlab创建RepoPage"><img class="post-bg" src="https://raw.githubusercontent.com/clvsit/markdown-image/master/eigen/tools/20200409231825.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="gitlab创建RepoPage"></a></div><div class="recent-post-info"><a class="article-title" href="/gitlab%E5%88%9B%E5%BB%BARepoPage/" title="gitlab创建RepoPage">gitlab创建RepoPage</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2020-04-09T14:44:50.000Z" title="发表于 2020-04-09 22:44:50">2020-04-09</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%B7%A5%E5%85%B7%E4%BB%8B%E7%BB%8D/">工具介绍</a></span></div><div class="content">步骤 1：创建项目打开 gitlab，点击上方的“+”号按钮，在显示的页面中填入 repo 相关的信息，例如 Project name 和 description。最后，点击 Create project 按钮。

步骤 2：添加 License在斌哥提供的 Youtubu 教程中需要创建 License，但我在实践的过程中发现该步骤非必需。
创建项目后会自动跳转到 repo 的 detail 页面，在该页面的正中央有一排按钮组，点击“Add License”。

然后，在调整后的页面中，选择 Template -&gt; Apache License 2.0。

步骤 3：添加 .gitlab-ci 配置文件完成 License 添加后，回退到 repo detail 页面，此时已有 LICENSE 文件。

然后按图所示，创建一个新的文件。

在创建新文件的页面中，先在左侧的 Template 下拉框中选择 .gitlab-ci.yml，然后在右侧的下拉框中选择 HTML。最后，点击 Commit changes。
此时，我们查看左侧栏的 CI&#x2F;CD -&gt; Pipe ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AEDA%EF%BC%9AEasy-Data-Augmentation-Techniques-for-Boosting-Performance-on-Text-Classification-Tasks/" title="论文阅读：EDA：Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks"><img class="post-bg" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/EDA%20Easy%20Data%20Augmentation%20Techniques%20for%20Boosting%20Performance%20on%20Text%20Classification%20Tasks/EDA%20Figure1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读：EDA：Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks"></a></div><div class="recent-post-info"><a class="article-title" href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AEDA%EF%BC%9AEasy-Data-Augmentation-Techniques-for-Boosting-Performance-on-Text-Classification-Tasks/" title="论文阅读：EDA：Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks">论文阅读：EDA：Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2020-03-03T05:58:57.000Z" title="发表于 2020-03-03 13:58:57">2020-03-03</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP/">NLP</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP/%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3/">数据相关</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP/%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/">数据增强</a></span></div><div class="content">EDA 包含四个简单但功能强大的操作：同义词替换，随机插入，随机交换和随机删除。在五个文本分类任务上，作者表明 EDA 可以提高卷积神经网络和循环神经网络的性能。EDA 对较小的数据集显示出特别强的结果。平均而言，在五个数据集上，仅使用 50％ 的可用训练集进行 EDA 训练就可以获得与使用所有可用数据进行的正常训练得到相同的准确性。此外，作者还进行了广泛的消融研究，并提出了实用的参数。
介绍机器学习和深度学习已在从情感分析到主题分类的任务上实现了很高的准确性，但是高性能通常取决于训练数据的大小和质量，而这往往很难收集。自动数据增强通常用于计算机视觉和语音，可以帮助训练更强大的模型，尤其是在使用较小的数据集时。但是，提出通用的语言转换规则比较困难，因此尚未充分探索 NLP 中的通用数据增强技术。
先前已有相关的研究提出了一些在 NLP 中用于数据增强的技术：

通过翻译来生成新的数据，例如将中文先翻译成英文，然后再将英文翻译成中文，此时就可以生成一份与原句近似语义的新句。
将数据噪声来平滑（smoothing）数据集。
将预测语言模型用作同义词替换。

尽管这些技术是有效的，但实际上它 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/%E6%96%B0%E8%AF%8D%E5%8F%91%E7%8E%B0/" title="新词发现"><img class="post-bg" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2NsdnNpdC9tYXJrZG93bi1pbWFnZS9tYXN0ZXIvbmxwL3dvcmRfZmluZC8yMDIwMDEwNzE2MDk1Ni5qcGc?x-oss-process=image/format,png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="新词发现"></a></div><div class="recent-post-info"><a class="article-title" href="/%E6%96%B0%E8%AF%8D%E5%8F%91%E7%8E%B0/" title="新词发现">新词发现</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2020-01-05T14:39:07.000Z" title="发表于 2020-01-05 22:39:07">2020-01-05</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP/">NLP</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP/%E5%85%B7%E4%BD%93%E4%BB%BB%E5%8A%A1/">具体任务</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP/%E5%85%B7%E4%BD%93%E4%BB%BB%E5%8A%A1/%E6%96%B0%E8%AF%8D%E5%8F%91%E7%8E%B0/">新词发现</a></span></div><div class="content">新词发现是 NLP 的基础任务之一，通过对已有语料进行挖掘，从中识别出新词。新词发现也可称为未登录词识别，严格来讲，新词是指随时代发展而新出现或旧词新用的词语。同时，我认为特定领域的专有名词也可归属于新词的范畴。何出此言呢？通常我们会很容易找到通用领域的词表，但要找到某个具体领域的专有名词则非常困难，因此特定领域的专有名词相对于通用领域的词语即为新词。换言之，“新”并非只是时间上的概念，同样可以迁移到领域或空间上。因此，新词发现不仅可以挖掘随时间变化而产生的新词，也可以挖掘不同领域的专有名词。
接下来，让我们开始一场新词发现的探索之旅吧。首先，对于“新词发现”这个标题，我们可将其拆分为“发现”和“新词”两个步骤：

“发现”：依据某种手段或方法，从文本中挖掘词语，组成新词表；
“新词”：借助挖掘得到的新词表，和之前已有的旧词表进行比对，不在旧词表中的词语即可认为是新词。

“新词发现”的难点主要在于“发现”的过程——如何从文本中挖掘到词语？那么有办法回避这个问题吗？让我们思索一下“新词”的过程：比对挖掘得到的新词表和旧词表，从代码的角度来说。
123for 新词 in 新词表:     ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/n-gram-%E8%AF%8D%E9%A2%91%E7%BB%9F%E8%AE%A1/" title="n-gram 词频统计">n-gram 词频统计</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2019-12-15T06:33:01.000Z" title="发表于 2019-12-15 14:33:01">2019-12-15</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/">数据处理</a></span></div><div class="content">n-gram 词频统计借助 sklearn 提供的 CountVectorizer 可以实现 n-gram 的词频统计。
实现过程首先，导入所需的包以及数据。
1234567from sklearn.feature_extraction.text import CountVectorizerfrom collections import ChainMapimport tqdmwith open(&quot;/nfs/users/chenxu/common_word_mining/dataset_word_cut_small.json&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as file:    content_list = json.load(file)

然后，调用 CountVectorizer，以获得每段文本的文本向量。
12vectorizer = CountVectorizer(token_pattern=r&quot;(?u)\b\w+\b&quot;, ngram_range=(2,2), min_df=5)X ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/%E5%87%BD%E6%95%B0%E6%B3%A8%E9%87%8A%E5%8F%8A%E5%85%B6%E5%A6%99%E7%94%A8/" title="函数注释及其妙用">函数注释及其妙用</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2019-10-24T06:39:15.000Z" title="发表于 2019-10-24 14:39:15">2019-10-24</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E7%BC%96%E7%A8%8B%E9%A3%8E%E6%A0%BC/">编程风格</a></span></div><div class="content">注释函数注释【示例】：
123456789101112@staticmethoddef report_info_add(report_a: dict, report_b: dict, key: str) -&gt; int:    &quot;&quot;&quot;    报告信息相加    :param report_a: dict 报告信息字典 A    :param report_b: dict 报告信息字典 B    :param key:      str  指定 key 值    :return: int 相加后的数值    &quot;&quot;&quot;    report_a_value = report_a[key] if key in report_a else 0    report_b_value = report_b[key] if key in report_b else 0    return report_a_value + report_b_value

在函数声明中 report_a 是参数，: 冒号后 dict 是参数 report_a 的 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/" title="模型优化"><img class="post-bg" src="https://img-blog.csdnimg.cn/img_convert/58657b64d39f6d8e48dd5f695b6065a4.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="模型优化"></a></div><div class="recent-post-info"><a class="article-title" href="/%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/" title="模型优化">模型优化</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2019-05-31T13:43:03.000Z" title="发表于 2019-05-31 21:43:03">2019-05-31</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/">模型优化</a></span></div><div class="content">优化是应用数学的一个分支，也是机器学习的核心组成部分。实际上，机器学习算法 &#x3D; 模型表征 + 模型评估 + 模型优化。其中，模型优化所做的事情就是在模型表征空间（假设空间）中找到模型评估指标最好的模型。需要注意的是不同的优化算法对应的模型表征和评估指标不尽相同。
先前，我很纠结是把损失函数放在模型评估中，还是放在模型优化这一篇博客中。准确地说，损失函数是用来作为模型评估的标准，不同的模型有不同的损失函数，例如逻辑回归使用交叉熵损失函数，线性回归使用均方误差损失函数。我们统计模型的损失，从而评估模型的优劣。因此，损失函数放到模型评估中是顺理成章的事情。但是，损失函数在模型优化中同样起到非常重要的作用。
很少有模型从一开始就是完美的，我们需要不断地优化模型，让模型逐渐达到理论最优值，而我们优化的目标就是损失函数——让损失函数达到最小值。从这角度来讲，将损失函数放到模型优化中似乎也非常有道理，因此我最终还是将损失函数放到模型优化这一篇博客中。
在本篇博客中除了讲述损失函数外，还包括机器学习中经典的优化算法、模型调参等相关知识，内容主要来源于博主阅读的书籍以及博主的个人领悟，若有偏驳 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/" title="模型评估"><img class="post-bg" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/ml/evaluation/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="模型评估"></a></div><div class="recent-post-info"><a class="article-title" href="/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/" title="模型评估">模型评估</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2019-05-30T14:24:53.000Z" title="发表于 2019-05-30 22:24:53">2019-05-30</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/">模型评估</a></span></div><div class="content">只有选择与问题相匹配的评估方法，才能快速地发现模型选择或训练过程中出现的问题，迭代地对模型进行优化。针对分类、排序、回归、序列预测等不同类型的机器学习问题，评估指标的选择也有所不同。知道每种评估指标的精确定义、有针对性地选择合适的评估指标、根据评估指标的反馈进行模型调整，这些都是机器学习在模型评估阶段的关键问题。
首先，我们先来了解一下关于模型评估的基础概念。
【误差(error)】：学习器的预测输出与样本的真实输出之间的差异。根据产生误差的数据集，可分为：

训练误差(training error)：又称为经验误差(empirical error)，学习器在训练集上的误差。
测试误差(test error)：学习器在测试集上的误差。
泛化误差(generalization error)：学习器在未知新样本上的误差。

需要注意的是，上述所说的“误差”均指误差期望，排除数据集大小的影响。
【目的】：得到泛化误差小的学习器。然而，事先并不知道新样本，实际能做的是努力使经验误差最小化。但需要明确一点，即使分类错误率为 0，精度为 100% 的学习器，也不一定能够在新样本上取得好的预测结果。 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0-%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F-%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98/" title="模型评估-性能度量(回归问题)"><img class="post-bg" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/ml/evaluation/%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F(%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98)%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="模型评估-性能度量(回归问题)"></a></div><div class="recent-post-info"><a class="article-title" href="/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0-%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F-%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98/" title="模型评估-性能度量(回归问题)">模型评估-性能度量(回归问题)</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2019-05-30T14:05:18.000Z" title="发表于 2019-05-30 22:05:18">2019-05-30</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/">模型评估</a></span></div><div class="content">对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是性能度量（performance measure）。
性能度量反映了任务需求，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果，这意味着模型的“好坏”是相对的，什么样的模型是好的，不仅取决于算法和数据，还决定于任务需求。
在预测任务中，给定数据集 $D &#x3D; {(x_1, y_1), (x_2, y_2), \ldots, (x_m, y_m)}$，其中 $y_i$ 是示例 $x_i$ 的真实标记。要估计学习器 f 的性能，就要把学习器预测结果 $f(x)$ 与真实标记 y 进行比较。
为了说明各性能度量指标，我们以波士顿房价数据集为例，模型选择决策树算法，通过 train_test_split() 划分数据集，最后评估各项性能指标。
123456789101112131415from sklearn.datasets import load_bostonfrom sklearn.tree import DecisionTreeRegressorfrom sk ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0-%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F-%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/" title="模型评估-性能度量(分类问题)"><img class="post-bg" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/ml/evaluation/%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F(%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98)%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="模型评估-性能度量(分类问题)"></a></div><div class="recent-post-info"><a class="article-title" href="/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0-%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F-%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/" title="模型评估-性能度量(分类问题)">模型评估-性能度量(分类问题)</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2019-05-30T04:28:36.000Z" title="发表于 2019-05-30 12:28:36">2019-05-30</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/">模型评估</a></span></div><div class="content">对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是性能度量（performance measure）。
性能度量反映了任务需求，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果，这意味着模型的“好坏”是相对的，什么样的模型是好的，不仅取决于算法和数据，还决定于任务需求。
在预测任务中，给定数据集 $D &#x3D; {(x_1, y_1), (x_2, y_2), …, (x_m, y_m)}$，其中 $y_i$ 是示例 $x_i$ 的真实标记。要估计学习器 f 的性能，就要把学习器预测结果 $f(x)$ 与真实标记 y 进行比较。
为了说明各性能度量指标，我们以鸢尾花数据集为例，模型选择决策树 CART 算法，通过 train_test_split() 划分数据集，最后评估各项性能指标。
123456789101112131415from sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifierfrom skle ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0-%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95/" title="模型评估-评估方法"><img class="post-bg" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/ml/evaluation/%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="模型评估-评估方法"></a></div><div class="recent-post-info"><a class="article-title" href="/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0-%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95/" title="模型评估-评估方法">模型评估-评估方法</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2019-05-29T13:41:26.000Z" title="发表于 2019-05-29 21:41:26">2019-05-29</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/">模型评估</a></span></div><div class="content">通常通过实验测试来对学习器的泛化误差进行评估并进而做出选择。为此，需要使用一个“测试集”（testing set）来测试学习器对新样本的判别能力，然后以测试集上的“测试误差”（testing error）作为泛化误差的近似。
【重要假设】：测试样本也是从样本真实分布中独立同分布采样而得。举个简单的例子，假设你要检验新研发的药对人的作用，你肯定是选择小白鼠，而不是红鲤鱼。因为红鲤鱼是鱼类，而小白鼠与人同属于哺乳动物。
下面将介绍机器学习中常用的评估方法，主要通过 sklearn 和鸢尾花数据集来进行说明。
1234567from sklearn.datasets import load_irisdataset = load_iris()data_iris = dataset.datatarget_iris = dataset.targetfeature_iris = dataset.feature_names

留出法留出法将测试数据集和训练数据集完全分开，采用测试数据集来评估算法模型。也就是说，我们可以简单地将原始数据集分为两部分：

第一部分作为训练数据集，用来训练算法生成模型；
第 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-%E8%BF%87%E6%BB%A4%E5%BC%8F%E9%80%89%E6%8B%A9/" title="特征选择-过滤式选择"><img class="post-bg" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/ml/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/feature%20selection/%E8%BF%87%E6%BB%A4%E5%BC%8F%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="特征选择-过滤式选择"></a></div><div class="recent-post-info"><a class="article-title" href="/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-%E8%BF%87%E6%BB%A4%E5%BC%8F%E9%80%89%E6%8B%A9/" title="特征选择-过滤式选择">特征选择-过滤式选择</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2019-05-28T15:09:11.000Z" title="发表于 2019-05-28 23:09:11">2019-05-28</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">特征工程</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/">特征选择</a></span></div><div class="content">过滤式方法先按照某种规则对数据集进行特征选择，然后再训练学习器，特征选择过程与后续学习器无关，这相当于先用特征选择过程对初始特征进行“过滤”，再用过滤后的特征来训练模型。
【某种规则】：按照发散性或相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，从而选择满足条件的特征。

特征的发散性：如果一个特征不发散，例如方差接近于 0，也就是说样本在该特征上基本没有差异，那么这个特征对于样本的区分并没有什么用。
特征与目标的相关性：特征与目标的相关性越高说明特征的变动对目标的影响较大，因此我们应当优先选择与目标相关性高的特征。

在后续所讲的方法中除方差选择法是基于特征发散性，其余方法均是从相关性考虑。

方差选择法计算各个特征的方差，然后根据阈值选择方差大于阈值的特征，或者指定待选择的特征数 k，然后选择 k 个最大方差的特征。
方差选择的依据是什么？举个极端的例子，在多分类问题中，如果某特征只有一个取值，那么该特征对分类结果没有任何意义，因为不管取什么值都为 1，单凭该特征是无法区分样本的分类。
需要注意的是，方差选择法只有在特征是离散型时才适用，如果是连续型则需要离散化后才能使用。 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-%E5%8C%85%E8%A3%B9%E5%BC%8F%E9%80%89%E6%8B%A9/" title="特征选择-包裹式选择"><img class="post-bg" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/ml/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/feature%20selection/%E5%8C%85%E8%A3%B9%E5%BC%8F%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="特征选择-包裹式选择"></a></div><div class="recent-post-info"><a class="article-title" href="/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-%E5%8C%85%E8%A3%B9%E5%BC%8F%E9%80%89%E6%8B%A9/" title="特征选择-包裹式选择">特征选择-包裹式选择</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2019-05-28T13:18:00.000Z" title="发表于 2019-05-28 21:18:00">2019-05-28</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">特征工程</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/">特征选择</a></span></div><div class="content">包裹式选择与过滤式选择不考虑后续学习器不同，直接把最终使用的学习器的性能作为特征子集的评价准则。换言之，包裹式选择的目的就是为给定学习器选择最有利于其性能、“量身定做”的特征子集。
【与过滤式选择的区别】：

包裹式选择方法直接针对给定学习器进行优化，因此，从最终学习器性能来看，包裹式选择比过滤式选择更好；
但另一方面，由于在特征选择过程中需多次训练学习器，因此包裹式选择的计算开销通常比过滤式选择大得多。


递归特征消除递归特征消除（Recursive Feature Elimination）使用一个基模型（学习器）来进行多轮训练，每轮训练后移除若干特征，再基于新的特征集进行下一轮训练。
【sklearn 官方解释】：对特征含有权重的预测模型，RFE 通过递归减少待考察特征集规模来选择特征。

首先，预测模型在原始特征集上进行训练，通过 coef_ 属性或 feature_importances_ 属性为每个特征指定一个权重；
然后，剔除那些权重绝对值较小的特征；
如此循环，直到剩余的特征数量达到所需的特征数量。

需要注意的是，RFE 的稳定性很大程度上取决于迭代时，底层使用的预测 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-%E5%B5%8C%E5%85%A5%E5%BC%8F%E9%80%89%E6%8B%A9/" title="特征选择-嵌入式选择"><img class="post-bg" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/ml/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/feature%20selection/%E5%B5%8C%E5%85%A5%E5%BC%8F%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="特征选择-嵌入式选择"></a></div><div class="recent-post-info"><a class="article-title" href="/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-%E5%B5%8C%E5%85%A5%E5%BC%8F%E9%80%89%E6%8B%A9/" title="特征选择-嵌入式选择">特征选择-嵌入式选择</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2019-05-28T07:37:56.000Z" title="发表于 2019-05-28 15:37:56">2019-05-28</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">特征工程</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/">特征选择</a></span></div><div class="content">嵌入式特征选择是将特征选择过程与学习器训练过程融为一体，两者在同一个优化过程中完成，即在学习器训练过程中自动地进行了特征选择。

基于惩罚项的特征选择法给定数据集 $D &#x3D; {(x_1, y_1), (x_2, y_2), \cdots, (x_n, y_n)}$，其中 $x \in R^d, y \in R$。我们考虑最简单的线性回归模型，以平方误差为损失函数，则优化目标为$$min_w \sum_{i&#x3D;1}^n(y_i - w^Tx_i)^2$$当样本特征很多，而样本数相对较少时，上式很容易陷入过拟合。为了缓解过拟合问题，可对上式引入正则化项。

使用 L2 范数正则化，则称为“岭回归”（ridge regression）。

$$min_w \sum_{i&#x3D;1}^n(y_i - w^Tx_i)^2 + \lambda ||w||_2^2$$通过引入 L2 范数正则化，确能显著降低过拟合的风险。

使用 L1 范数正则化，则称为 LASSO（Least Absolute Shrinkage and Selection Operator）。

$$min ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/" title="特征选择"><img class="post-bg" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/ml/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/feature%20selection/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="特征选择"></a></div><div class="recent-post-info"><a class="article-title" href="/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/" title="特征选择">特征选择</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2019-05-26T14:46:34.000Z" title="发表于 2019-05-26 22:46:34">2019-05-26</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">特征工程</a></span></div><div class="content">对一个学习任务来说，给定属性集，其中有些属性可能很关键、很有用，另一些属性则可能没什么用，我们将属性称为“特征”（feature），对当前学习任务有用的属性称为“相关特征”（relevant feature）、没什么用的属性称为“无关特征”（irrelavant feature）。从给定的特征集合中选择出相关特征子集的过程，称为“特征选择”（feature selection）。
【注意】：

特征的相关与无关是相对当前学习任务而言，若更换学习任务则有可能使得原本相关特征变为无关特征。例如姓名特征对于预测年龄几乎没有什么作用，但对于预测性别则有一定的参考价值。
特征选择过程必须确保不丢失重要特征，否则后续学习过程会因为重要信息的缺失而无法获得好的性能。

特征选择是一个重要的“数据预处理”过程，在现实机器学习任务中，获得数据之后通常先进行特征选择，此后再训练学习器，那么为什么要进行特征选择呢？

首先，我们在现实任务中经常会遇到维数灾难问题，这是由于特征过多而造成的，若能从中选择出重要的特征，使得后续学习过程仅需在一部分特征上构建模型，则维数灾难问题会大为减轻。从这个意义上来说，特征选 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/%E7%89%B9%E5%BE%81%E5%BD%92%E4%B8%80%E5%8C%96/" title="特征归一化"><img class="post-bg" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/ml/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E6%95%B0%E6%8D%AE%E5%BD%92%E4%B8%80%E5%8C%96%E5%AF%B9%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%94%B6%E6%95%9B%E9%80%9F%E5%BA%A6%E4%BA%A7%E7%94%9F%E7%9A%84%E5%BD%B1%E5%93%8D-2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="特征归一化"></a></div><div class="recent-post-info"><a class="article-title" href="/%E7%89%B9%E5%BE%81%E5%BD%92%E4%B8%80%E5%8C%96/" title="特征归一化">特征归一化</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2019-05-19T14:31:06.000Z" title="发表于 2019-05-19 22:31:06">2019-05-19</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">特征工程</a></span></div><div class="content">概念消除数据特征之间的量纲影响，可以将所有的特征都统一到一个大致相同的数值区间内，使得不同指标之间具有可比性。例如，分析一个人的身高和年龄对健康的影响，通常身高用 cm 作为单位，而年龄用岁作为单位，那么身高特征会大致在 160180 cm 的数值范围内，年龄特征会在 1100 岁的范围内，分析的结果显然会倾向于数值差别比较大的身高特征。想要得到更为准确的结果，就需要对数据进行特征归一化（Normalization）处理，使各指标处于同一数值量级，以便进行分析。特征归一化有时被称为特征缩放或特征规范化。
【量纲（dimension）】：物理量固有的、可度量的物理属性。例如，人的身高、体重和年龄等。
【问答 QA】：

问：为什么对拥有不同数值量级特征的数据集进行分析，其结果会倾向于数值差别较大的特征？
答：具体内容可以参考这篇博文 K-近邻算法 问题 QA 中的“为什么要做数据归一化”。

通常对于需要计算数据集特征距离的模型，我们都需要提前对数据集做特征归一化，尤其是数值型的数据。
常用方法
线性函数归一化（Min-Max Scaling）：又称为 min-max 缩放，是对原始数据 ...</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/4/#content-inner"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/#content-inner">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/#content-inner">6</a><a class="extend next" rel="next" href="/page/6/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">clvsit</div><div class="author-info__description">人生不是戏剧，而我亦非主角</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">83</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">67</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/clvsit"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/clvsit" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">知乎和 CSDN 同名 clvsit，目前在将本地的笔记逐步迁移到这，所以会看到过去日期的文章不断增多</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AEnhancing-Retrieval-and-Managing-Retrieval-A-Four-Module-Synergy-for-Improved-Quality-and-Efficiency-in-RAG-Systems/" title="论文阅读：Enhancing Retrieval and Managing Retrieval: A Four-Module Synergy for Improved Quality and Efficiency in RAG Systems"><img src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Enhancing%20Retrieval%20and%20Managing%20Retrieval%20A%20Four-Module%20Synergy%20for%20Improved%20Quality%20and%20Efficiency%20in%20RAG%20Systems/Figure%201.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读：Enhancing Retrieval and Managing Retrieval: A Four-Module Synergy for Improved Quality and Efficiency in RAG Systems"/></a><div class="content"><a class="title" href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AEnhancing-Retrieval-and-Managing-Retrieval-A-Four-Module-Synergy-for-Improved-Quality-and-Efficiency-in-RAG-Systems/" title="论文阅读：Enhancing Retrieval and Managing Retrieval: A Four-Module Synergy for Improved Quality and Efficiency in RAG Systems">论文阅读：Enhancing Retrieval and Managing Retrieval: A Four-Module Synergy for Improved Quality and Efficiency in RAG Systems</a><time datetime="2024-07-27T12:14:01.000Z" title="发表于 2024-07-27 20:14:01">2024-07-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADQ-LoRe%EF%BC%9ADual-Queries-with-Low-Rank-Approximation-Re-ranking-for-In-Context-Learning/" title="论文阅读：DQ-LoRe：Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning"><img src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/DQ-LoRe%20Dual%20Queries%20with%20Low%20Rank%20Approximation%20Re-ranking%20for%20In-Context%20Learning/Figure%201.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读：DQ-LoRe：Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning"/></a><div class="content"><a class="title" href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ADQ-LoRe%EF%BC%9ADual-Queries-with-Low-Rank-Approximation-Re-ranking-for-In-Context-Learning/" title="论文阅读：DQ-LoRe：Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning">论文阅读：DQ-LoRe：Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning</a><time datetime="2024-07-17T15:22:10.000Z" title="发表于 2024-07-17 23:22:10">2024-07-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ASpeculative-RAG-Enhancing-Retrieval-Augmented-Generation-through-Drafting/" title="论文阅读：Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting"><img src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Speculative%20RAG%20Enhancing%20Retrieval%20Augmented%20Generation%20through%20Drafting/Figure%201.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读：Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting"/></a><div class="content"><a class="title" href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ASpeculative-RAG-Enhancing-Retrieval-Augmented-Generation-through-Drafting/" title="论文阅读：Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting">论文阅读：Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting</a><time datetime="2024-07-16T14:21:52.000Z" title="发表于 2024-07-16 22:21:52">2024-07-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ABe-like-a-Goldfish-Don-t-Memorize-Mitigating-Memorization-in-Generative-LLMs/" title="论文阅读：Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs"><img src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Be%20like%20a%20Goldfish%20Dont%20Memorize%20Mitigating%20Memorization%20in%20Generative%20LLMs/Figure%201.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读：Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs"/></a><div class="content"><a class="title" href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ABe-like-a-Goldfish-Don-t-Memorize-Mitigating-Memorization-in-Generative-LLMs/" title="论文阅读：Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs">论文阅读：Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs</a><time datetime="2024-07-08T15:12:03.000Z" title="发表于 2024-07-08 23:12:03">2024-07-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/AI-%E6%83%85%E6%84%9F%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%E4%B9%8B%E6%97%85%E2%80%94%E2%80%94%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87%E6%94%B6%E9%9B%86/" title="AI-情感聊天机器人之旅——相关论文收集">AI-情感聊天机器人之旅——相关论文收集</a><time datetime="2024-06-20T13:15:26.000Z" title="发表于 2024-06-20 21:15:26">2024-06-20</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            <a class="card-more-btn" href="/categories/" title="查看更多">
    <i class="fas fa-angle-right"></i></a>
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/LLM/"><span class="card-category-list-name">LLM</span><span class="card-category-list-count">15</span></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/LLM/LLM-%E5%8E%8B%E7%BC%A9/"><span class="card-category-list-name">LLM 压缩</span><span class="card-category-list-count">1</span></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/LLM/LLM-%E5%8E%8B%E7%BC%A9/kv-cache-%E5%8E%8B%E7%BC%A9/"><span class="card-category-list-name">kv cache 压缩</span><span class="card-category-list-count">1</span></a></li></ul></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/LLM/SFT/"><span class="card-category-list-name">SFT</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/LLM/%E6%8E%A8%E7%90%86%E7%9B%B8%E5%85%B3/"><span class="card-category-list-name">推理相关</span><span class="card-category-list-count">4</span></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/LLM/%E6%8E%A8%E7%90%86%E7%9B%B8%E5%85%B3/%E6%8E%A8%E7%90%86%E6%9C%8D%E5%8A%A1/"><span class="card-category-list-name">推理服务</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/LLM/%E6%8E%A8%E7%90%86%E7%9B%B8%E5%85%B3/%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/"><span class="card-category-list-name">推理框架</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/LLM/%E6%8E%A8%E7%90%86%E7%9B%B8%E5%85%B3/%E6%98%BE%E5%8D%A1%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%84/"><span class="card-category-list-name">显卡性能测评</span><span class="card-category-list-count">1</span></a></li></ul></li></ul></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/long-context/" style="font-size: 1.18em; color: #999ca1">long context</a> <a href="/tags/RAG/" style="font-size: 1.42em; color: #99a6b7">RAG</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E6%9E%84%E9%80%A0/" style="font-size: 1.1em; color: #999">数据构造</a> <a href="/tags/LLM-%E6%8E%A8%E7%90%86%E7%9B%B8%E5%85%B3/" style="font-size: 1.18em; color: #999ca1">LLM 推理相关</a> <a href="/tags/%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB/" style="font-size: 1.1em; color: #999">层次聚类</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 1.18em; color: #999ca1">数据增强</a> <a href="/tags/SFT/" style="font-size: 1.1em; color: #999">SFT</a> <a href="/tags/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/" style="font-size: 1.1em; color: #999">推理加速</a> <a href="/tags/prompt-%E5%B7%A5%E7%A8%8B/" style="font-size: 1.34em; color: #99a3b0">prompt 工程</a> <a href="/tags/%E5%B7%A5%E4%BD%9C%E5%86%85%E5%AE%B9/" style="font-size: 1.26em; color: #999fa8">工作内容</a> <a href="/tags/%E5%8F%82%E6%95%B0%E4%B8%8E%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90/" style="font-size: 1.1em; color: #999">参数与显存分析</a> <a href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/" style="font-size: 1.1em; color: #999">文本分类</a> <a href="/tags/prompt-%E5%8E%8B%E7%BC%A9/" style="font-size: 1.1em; color: #999">prompt 压缩</a> <a href="/tags/%E6%8F%90%E5%89%8D%E9%80%80%E5%87%BA/" style="font-size: 1.18em; color: #999ca1">提前退出</a> <a href="/tags/%E6%B7%B7%E5%90%88%E6%A3%80%E7%B4%A2/" style="font-size: 1.1em; color: #999">混合检索</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%AD%90%E9%9B%86%E6%8C%91%E9%80%89/" style="font-size: 1.1em; color: #999">数据子集挑选</a> <a href="/tags/vLLM-%E6%A1%86%E6%9E%B6/" style="font-size: 1.18em; color: #999ca1">vLLM 框架</a> <a href="/tags/kv-cache-%E5%8E%8B%E7%BC%A9/" style="font-size: 1.1em; color: #999">kv cache 压缩</a> <a href="/tags/%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/" style="font-size: 1.1em; color: #999">子词模型</a> <a href="/tags/%E9%87%8D%E6%8E%92/" style="font-size: 1.1em; color: #999">重排</a> <a href="/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" style="font-size: 1.18em; color: #999ca1">注意力机制</a> <a href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" style="font-size: 1.5em; color: #99a9bf">论文阅读</a> <a href="/tags/%E6%9F%A5%E8%AF%A2%E5%8F%98%E6%8D%A2/" style="font-size: 1.1em; color: #999">查询变换</a> <a href="/tags/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/" style="font-size: 1.26em; color: #999fa8">位置编码</a> <a href="/tags/LLM-%E6%8E%A8%E7%90%86%E6%9C%8D%E5%8A%A1/" style="font-size: 1.1em; color: #999">LLM 推理服务</a> <a href="/tags/%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA/" style="font-size: 1.1em; color: #999">检索增强</a> <a href="/tags/%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6/" style="font-size: 1.1em; color: #999">推理框架</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%90%88%E6%88%90/" style="font-size: 1.18em; color: #999ca1">数据合成</a> <a href="/tags/%E7%A1%AC%E4%BB%B6/" style="font-size: 1.18em; color: #999ca1">硬件</a> <a href="/tags/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F/" style="font-size: 1.18em; color: #999ca1">知识蒸馏</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83/" style="font-size: 1.1em; color: #999">分布式并行训练</a> <a href="/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/" style="font-size: 1.26em; color: #999fa8">预训练模型</a> <a href="/tags/%E8%A7%A3%E7%A0%81%E7%AD%96%E7%95%A5/" style="font-size: 1.1em; color: #999">解码策略</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>归档</span><a class="card-more-btn" href="/archives/" title="查看更多">
    <i class="fas fa-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/07/"><span class="card-archive-list-date">七月 2024</span><span class="card-archive-list-count">4</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/06/"><span class="card-archive-list-date">六月 2024</span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/05/"><span class="card-archive-list-date">五月 2024</span><span class="card-archive-list-count">9</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/04/"><span class="card-archive-list-date">四月 2024</span><span class="card-archive-list-count">7</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/03/"><span class="card-archive-list-date">三月 2024</span><span class="card-archive-list-count">7</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/02/"><span class="card-archive-list-date">二月 2024</span><span class="card-archive-list-count">5</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/01/"><span class="card-archive-list-date">一月 2024</span><span class="card-archive-list-count">6</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/12/"><span class="card-archive-list-date">十二月 2023</span><span class="card-archive-list-count">1</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">83</div></div><div class="webinfo-item"><div class="item-name">本站总字数 :</div><div class="item-count">227.4k</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2024-07-27T12:52:25.556Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/banner.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By clvsit</div><div class="footer_custom_text">城南芳草城北溪，池塘烟柳归鸟栖。忽听窗外风吹雨，一梦花落月近西。</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>window.typedJSFn = {
  init: (str) => {
    window.typed = new Typed('#subtitle', Object.assign({
      strings: str,
      startDelay: 300,
      typeSpeed: 150,
      loop: true,
      backSpeed: 50,
    }, null))
  },
  run: (subtitleType) => {
    if (true) {
      if (typeof Typed === 'function') {
        subtitleType()
      } else {
        getScript('https://cdn.jsdelivr.net/npm/typed.js@2.1.0/dist/typed.umd.min.js').then(subtitleType)
      }
    } else {
      subtitleType()
    }
  }
}
</script><script>function subtitleType () {
  if (true) {
    typedJSFn.init("sadasd")
  } else {
    document.getElementById("subtitle").textContent = "s"
  }
}
typedJSFn.run(subtitleType)</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"]):not([href="/music/"]):not([href="/no-pjax/"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener
  btf.removeGlobalFnEvent('pjax')
  btf.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>