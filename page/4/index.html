<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>clvsit 个人博客</title><meta name="author" content="clvsit"><meta name="copyright" content="clvsit"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="人生不是戏剧，而我亦非主角">
<meta property="og:type" content="website">
<meta property="og:title" content="clvsit 个人博客">
<meta property="og:url" content="https://clvsit.github.io/page/4/">
<meta property="og:site_name" content="clvsit 个人博客">
<meta property="og:description" content="人生不是戏剧，而我亦非主角">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://clvsit.github.io/img/avatar.jpg">
<meta property="article:author" content="clvsit">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://clvsit.github.io/img/avatar.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://clvsit.github.io/page/4/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'clvsit 个人博客',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2024-06-27 00:05:20'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.1.1"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">65</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">20</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">51</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image: url('/img/banner.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="clvsit 个人博客"><span class="site-name">clvsit 个人博客</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">clvsit 个人博客</h1><div id="site-subtitle"><span id="subtitle"></span></div><div id="site_social_icons"><a class="social-icon" href="https://github.com/clvsit" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left"><a href="/gitlab%E5%88%9B%E5%BB%BARepoPage/" title="gitlab创建RepoPage"><img class="post-bg" src="https://raw.githubusercontent.com/clvsit/markdown-image/master/eigen/tools/20200409231825.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="gitlab创建RepoPage"></a></div><div class="recent-post-info"><a class="article-title" href="/gitlab%E5%88%9B%E5%BB%BARepoPage/" title="gitlab创建RepoPage">gitlab创建RepoPage</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2020-04-09T14:44:50.000Z" title="发表于 2020-04-09 22:44:50">2020-04-09</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%B7%A5%E5%85%B7%E4%BB%8B%E7%BB%8D/">工具介绍</a></span></div><div class="content">步骤 1：创建项目打开 gitlab，点击上方的“+”号按钮，在显示的页面中填入 repo 相关的信息，例如 Project name 和 description。最后，点击 Create project 按钮。

步骤 2：添加 License在斌哥提供的 Youtubu 教程中需要创建 License，但我在实践的过程中发现该步骤非必需。
创建项目后会自动跳转到 repo 的 detail 页面，在该页面的正中央有一排按钮组，点击“Add License”。

然后，在调整后的页面中，选择 Template -&gt; Apache License 2.0。

步骤 3：添加 .gitlab-ci 配置文件完成 License 添加后，回退到 repo detail 页面，此时已有 LICENSE 文件。

然后按图所示，创建一个新的文件。

在创建新文件的页面中，先在左侧的 Template 下拉框中选择 .gitlab-ci.yml，然后在右侧的下拉框中选择 HTML。最后，点击 Commit changes。
此时，我们查看左侧栏的 CI&#x2F;CD -&gt; Pipe ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/%E6%96%B0%E8%AF%8D%E5%8F%91%E7%8E%B0/" title="新词发现"><img class="post-bg" src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2NsdnNpdC9tYXJrZG93bi1pbWFnZS9tYXN0ZXIvbmxwL3dvcmRfZmluZC8yMDIwMDEwNzE2MDk1Ni5qcGc?x-oss-process=image/format,png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="新词发现"></a></div><div class="recent-post-info"><a class="article-title" href="/%E6%96%B0%E8%AF%8D%E5%8F%91%E7%8E%B0/" title="新词发现">新词发现</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2020-01-05T14:39:07.000Z" title="发表于 2020-01-05 22:39:07">2020-01-05</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a></span></div><div class="content">新词发现是 NLP 的基础任务之一，通过对已有语料进行挖掘，从中识别出新词。新词发现也可称为未登录词识别，严格来讲，新词是指随时代发展而新出现或旧词新用的词语。同时，我认为特定领域的专有名词也可归属于新词的范畴。何出此言呢？通常我们会很容易找到通用领域的词表，但要找到某个具体领域的专有名词则非常困难，因此特定领域的专有名词相对于通用领域的词语即为新词。换言之，“新”并非只是时间上的概念，同样可以迁移到领域或空间上。因此，新词发现不仅可以挖掘随时间变化而产生的新词，也可以挖掘不同领域的专有名词。
接下来，让我们开始一场新词发现的探索之旅吧。首先，对于“新词发现”这个标题，我们可将其拆分为“发现”和“新词”两个步骤：

“发现”：依据某种手段或方法，从文本中挖掘词语，组成新词表；
“新词”：借助挖掘得到的新词表，和之前已有的旧词表进行比对，不在旧词表中的词语即可认为是新词。

“新词发现”的难点主要在于“发现”的过程——如何从文本中挖掘到词语？那么有办法回避这个问题吗？让我们思索一下“新词”的过程：比对挖掘得到的新词表和旧词表，从代码的角度来说。
123for 新词 in 新词表:     ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/n-gram-%E8%AF%8D%E9%A2%91%E7%BB%9F%E8%AE%A1/" title="n-gram 词频统计">n-gram 词频统计</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2019-12-15T06:33:01.000Z" title="发表于 2019-12-15 14:33:01">2019-12-15</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/">数据处理</a></span></div><div class="content">n-gram 词频统计借助 sklearn 提供的 CountVectorizer 可以实现 n-gram 的词频统计。
实现过程首先，导入所需的包以及数据。
1234567from sklearn.feature_extraction.text import CountVectorizerfrom collections import ChainMapimport tqdmwith open(&quot;/nfs/users/chenxu/common_word_mining/dataset_word_cut_small.json&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as file:    content_list = json.load(file)

然后，调用 CountVectorizer，以获得每段文本的文本向量。
12vectorizer = CountVectorizer(token_pattern=r&quot;(?u)\b\w+\b&quot;, ngram_range=(2,2), min_df=5)X ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/%E5%87%BD%E6%95%B0%E6%B3%A8%E9%87%8A%E5%8F%8A%E5%85%B6%E5%A6%99%E7%94%A8/" title="函数注释及其妙用">函数注释及其妙用</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2019-10-24T06:39:15.000Z" title="发表于 2019-10-24 14:39:15">2019-10-24</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E7%BC%96%E7%A8%8B%E9%A3%8E%E6%A0%BC/">编程风格</a></span></div><div class="content">注释函数注释【示例】：
123456789101112@staticmethoddef report_info_add(report_a: dict, report_b: dict, key: str) -&gt; int:    &quot;&quot;&quot;    报告信息相加    :param report_a: dict 报告信息字典 A    :param report_b: dict 报告信息字典 B    :param key:      str  指定 key 值    :return: int 相加后的数值    &quot;&quot;&quot;    report_a_value = report_a[key] if key in report_a else 0    report_b_value = report_b[key] if key in report_b else 0    return report_a_value + report_b_value

在函数声明中 report_a 是参数，: 冒号后 dict 是参数 report_a 的 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/" title="模型优化"><img class="post-bg" src="https://img-blog.csdnimg.cn/img_convert/58657b64d39f6d8e48dd5f695b6065a4.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="模型优化"></a></div><div class="recent-post-info"><a class="article-title" href="/%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/" title="模型优化">模型优化</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2019-05-31T13:43:03.000Z" title="发表于 2019-05-31 21:43:03">2019-05-31</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96/">模型优化</a></span></div><div class="content">优化是应用数学的一个分支，也是机器学习的核心组成部分。实际上，机器学习算法 &#x3D; 模型表征 + 模型评估 + 模型优化。其中，模型优化所做的事情就是在模型表征空间（假设空间）中找到模型评估指标最好的模型。需要注意的是不同的优化算法对应的模型表征和评估指标不尽相同。
先前，我很纠结是把损失函数放在模型评估中，还是放在模型优化这一篇博客中。准确地说，损失函数是用来作为模型评估的标准，不同的模型有不同的损失函数，例如逻辑回归使用交叉熵损失函数，线性回归使用均方误差损失函数。我们统计模型的损失，从而评估模型的优劣。因此，损失函数放到模型评估中是顺理成章的事情。但是，损失函数在模型优化中同样起到非常重要的作用。
很少有模型从一开始就是完美的，我们需要不断地优化模型，让模型逐渐达到理论最优值，而我们优化的目标就是损失函数——让损失函数达到最小值。从这角度来讲，将损失函数放到模型优化中似乎也非常有道理，因此我最终还是将损失函数放到模型优化这一篇博客中。
在本篇博客中除了讲述损失函数外，还包括机器学习中经典的优化算法、模型调参等相关知识，内容主要来源于博主阅读的书籍以及博主的个人领悟，若有偏驳 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/" title="模型评估"><img class="post-bg" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/ml/evaluation/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="模型评估"></a></div><div class="recent-post-info"><a class="article-title" href="/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/" title="模型评估">模型评估</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2019-05-30T14:24:53.000Z" title="发表于 2019-05-30 22:24:53">2019-05-30</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/">模型评估</a></span></div><div class="content">只有选择与问题相匹配的评估方法，才能快速地发现模型选择或训练过程中出现的问题，迭代地对模型进行优化。针对分类、排序、回归、序列预测等不同类型的机器学习问题，评估指标的选择也有所不同。知道每种评估指标的精确定义、有针对性地选择合适的评估指标、根据评估指标的反馈进行模型调整，这些都是机器学习在模型评估阶段的关键问题。
首先，我们先来了解一下关于模型评估的基础概念。
【误差(error)】：学习器的预测输出与样本的真实输出之间的差异。根据产生误差的数据集，可分为：

训练误差(training error)：又称为经验误差(empirical error)，学习器在训练集上的误差。
测试误差(test error)：学习器在测试集上的误差。
泛化误差(generalization error)：学习器在未知新样本上的误差。

需要注意的是，上述所说的“误差”均指误差期望，排除数据集大小的影响。
【目的】：得到泛化误差小的学习器。然而，事先并不知道新样本，实际能做的是努力使经验误差最小化。但需要明确一点，即使分类错误率为 0，精度为 100% 的学习器，也不一定能够在新样本上取得好的预测结果。 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0-%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F-%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98/" title="模型评估-性能度量(回归问题)"><img class="post-bg" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/ml/evaluation/%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F(%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98)%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="模型评估-性能度量(回归问题)"></a></div><div class="recent-post-info"><a class="article-title" href="/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0-%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F-%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98/" title="模型评估-性能度量(回归问题)">模型评估-性能度量(回归问题)</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2019-05-30T14:05:18.000Z" title="发表于 2019-05-30 22:05:18">2019-05-30</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/">模型评估</a></span></div><div class="content">对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是性能度量（performance measure）。
性能度量反映了任务需求，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果，这意味着模型的“好坏”是相对的，什么样的模型是好的，不仅取决于算法和数据，还决定于任务需求。
在预测任务中，给定数据集 $D &#x3D; {(x_1, y_1), (x_2, y_2), \ldots, (x_m, y_m)}$，其中 $y_i$ 是示例 $x_i$ 的真实标记。要估计学习器 f 的性能，就要把学习器预测结果 $f(x)$ 与真实标记 y 进行比较。
为了说明各性能度量指标，我们以波士顿房价数据集为例，模型选择决策树算法，通过 train_test_split() 划分数据集，最后评估各项性能指标。
123456789101112131415from sklearn.datasets import load_bostonfrom sklearn.tree import DecisionTreeRegressorfrom sk ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0-%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F-%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/" title="模型评估-性能度量(分类问题)"><img class="post-bg" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/ml/evaluation/%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F(%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98)%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="模型评估-性能度量(分类问题)"></a></div><div class="recent-post-info"><a class="article-title" href="/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0-%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F-%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/" title="模型评估-性能度量(分类问题)">模型评估-性能度量(分类问题)</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2019-05-30T04:28:36.000Z" title="发表于 2019-05-30 12:28:36">2019-05-30</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/">模型评估</a></span></div><div class="content">对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要有衡量模型泛化能力的评价标准，这就是性能度量（performance measure）。
性能度量反映了任务需求，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果，这意味着模型的“好坏”是相对的，什么样的模型是好的，不仅取决于算法和数据，还决定于任务需求。
在预测任务中，给定数据集 $D &#x3D; {(x_1, y_1), (x_2, y_2), …, (x_m, y_m)}$，其中 $y_i$ 是示例 $x_i$ 的真实标记。要估计学习器 f 的性能，就要把学习器预测结果 $f(x)$ 与真实标记 y 进行比较。
为了说明各性能度量指标，我们以鸢尾花数据集为例，模型选择决策树 CART 算法，通过 train_test_split() 划分数据集，最后评估各项性能指标。
123456789101112131415from sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifierfrom skle ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0-%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95/" title="模型评估-评估方法"><img class="post-bg" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/ml/evaluation/%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="模型评估-评估方法"></a></div><div class="recent-post-info"><a class="article-title" href="/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0-%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95/" title="模型评估-评估方法">模型评估-评估方法</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2019-05-29T13:41:26.000Z" title="发表于 2019-05-29 21:41:26">2019-05-29</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/">模型评估</a></span></div><div class="content">通常通过实验测试来对学习器的泛化误差进行评估并进而做出选择。为此，需要使用一个“测试集”（testing set）来测试学习器对新样本的判别能力，然后以测试集上的“测试误差”（testing error）作为泛化误差的近似。
【重要假设】：测试样本也是从样本真实分布中独立同分布采样而得。举个简单的例子，假设你要检验新研发的药对人的作用，你肯定是选择小白鼠，而不是红鲤鱼。因为红鲤鱼是鱼类，而小白鼠与人同属于哺乳动物。
下面将介绍机器学习中常用的评估方法，主要通过 sklearn 和鸢尾花数据集来进行说明。
1234567from sklearn.datasets import load_irisdataset = load_iris()data_iris = dataset.datatarget_iris = dataset.targetfeature_iris = dataset.feature_names

留出法留出法将测试数据集和训练数据集完全分开，采用测试数据集来评估算法模型。也就是说，我们可以简单地将原始数据集分为两部分：

第一部分作为训练数据集，用来训练算法生成模型；
第 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-%E8%BF%87%E6%BB%A4%E5%BC%8F%E9%80%89%E6%8B%A9/" title="特征选择-过滤式选择"><img class="post-bg" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/ml/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/feature%20selection/%E8%BF%87%E6%BB%A4%E5%BC%8F%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="特征选择-过滤式选择"></a></div><div class="recent-post-info"><a class="article-title" href="/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-%E8%BF%87%E6%BB%A4%E5%BC%8F%E9%80%89%E6%8B%A9/" title="特征选择-过滤式选择">特征选择-过滤式选择</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2019-05-28T15:09:11.000Z" title="发表于 2019-05-28 23:09:11">2019-05-28</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">特征工程</a></span></div><div class="content">过滤式方法先按照某种规则对数据集进行特征选择，然后再训练学习器，特征选择过程与后续学习器无关，这相当于先用特征选择过程对初始特征进行“过滤”，再用过滤后的特征来训练模型。
【某种规则】：按照发散性或相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，从而选择满足条件的特征。

特征的发散性：如果一个特征不发散，例如方差接近于 0，也就是说样本在该特征上基本没有差异，那么这个特征对于样本的区分并没有什么用。
特征与目标的相关性：特征与目标的相关性越高说明特征的变动对目标的影响较大，因此我们应当优先选择与目标相关性高的特征。

在后续所讲的方法中除方差选择法是基于特征发散性，其余方法均是从相关性考虑。

方差选择法计算各个特征的方差，然后根据阈值选择方差大于阈值的特征，或者指定待选择的特征数 k，然后选择 k 个最大方差的特征。
方差选择的依据是什么？举个极端的例子，在多分类问题中，如果某特征只有一个取值，那么该特征对分类结果没有任何意义，因为不管取什么值都为 1，单凭该特征是无法区分样本的分类。
需要注意的是，方差选择法只有在特征是离散型时才适用，如果是连续型则需要离散化后才能使用。 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-%E5%8C%85%E8%A3%B9%E5%BC%8F%E9%80%89%E6%8B%A9/" title="特征选择-包裹式选择"><img class="post-bg" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/ml/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/feature%20selection/%E5%8C%85%E8%A3%B9%E5%BC%8F%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="特征选择-包裹式选择"></a></div><div class="recent-post-info"><a class="article-title" href="/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-%E5%8C%85%E8%A3%B9%E5%BC%8F%E9%80%89%E6%8B%A9/" title="特征选择-包裹式选择">特征选择-包裹式选择</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2019-05-28T13:18:00.000Z" title="发表于 2019-05-28 21:18:00">2019-05-28</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/">特征选择</a></span></div><div class="content">包裹式选择与过滤式选择不考虑后续学习器不同，直接把最终使用的学习器的性能作为特征子集的评价准则。换言之，包裹式选择的目的就是为给定学习器选择最有利于其性能、“量身定做”的特征子集。
【与过滤式选择的区别】：

包裹式选择方法直接针对给定学习器进行优化，因此，从最终学习器性能来看，包裹式选择比过滤式选择更好；
但另一方面，由于在特征选择过程中需多次训练学习器，因此包裹式选择的计算开销通常比过滤式选择大得多。


递归特征消除递归特征消除（Recursive Feature Elimination）使用一个基模型（学习器）来进行多轮训练，每轮训练后移除若干特征，再基于新的特征集进行下一轮训练。
【sklearn 官方解释】：对特征含有权重的预测模型，RFE 通过递归减少待考察特征集规模来选择特征。

首先，预测模型在原始特征集上进行训练，通过 coef_ 属性或 feature_importances_ 属性为每个特征指定一个权重；
然后，剔除那些权重绝对值较小的特征；
如此循环，直到剩余的特征数量达到所需的特征数量。

需要注意的是，RFE 的稳定性很大程度上取决于迭代时，底层使用的预测 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-%E5%B5%8C%E5%85%A5%E5%BC%8F%E9%80%89%E6%8B%A9/" title="特征选择-嵌入式选择"><img class="post-bg" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/ml/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/feature%20selection/%E5%B5%8C%E5%85%A5%E5%BC%8F%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="特征选择-嵌入式选择"></a></div><div class="recent-post-info"><a class="article-title" href="/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-%E5%B5%8C%E5%85%A5%E5%BC%8F%E9%80%89%E6%8B%A9/" title="特征选择-嵌入式选择">特征选择-嵌入式选择</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2019-05-28T07:37:56.000Z" title="发表于 2019-05-28 15:37:56">2019-05-28</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">特征工程</a></span></div><div class="content">嵌入式特征选择是将特征选择过程与学习器训练过程融为一体，两者在同一个优化过程中完成，即在学习器训练过程中自动地进行了特征选择。

基于惩罚项的特征选择法给定数据集 $D &#x3D; {(x_1, y_1), (x_2, y_2), \cdots, (x_n, y_n)}$，其中 $x \in R^d, y \in R$。我们考虑最简单的线性回归模型，以平方误差为损失函数，则优化目标为$$min_w \sum_{i&#x3D;1}^n(y_i - w^Tx_i)^2$$当样本特征很多，而样本数相对较少时，上式很容易陷入过拟合。为了缓解过拟合问题，可对上式引入正则化项。

使用 L2 范数正则化，则称为“岭回归”（ridge regression）。

$$min_w \sum_{i&#x3D;1}^n(y_i - w^Tx_i)^2 + \lambda ||w||_2^2$$通过引入 L2 范数正则化，确能显著降低过拟合的风险。

使用 L1 范数正则化，则称为 LASSO（Least Absolute Shrinkage and Selection Operator）。

$$min ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/" title="特征选择"><img class="post-bg" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/ml/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/feature%20selection/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="特征选择"></a></div><div class="recent-post-info"><a class="article-title" href="/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/" title="特征选择">特征选择</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2019-05-26T14:46:34.000Z" title="发表于 2019-05-26 22:46:34">2019-05-26</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">特征工程</a></span></div><div class="content">对一个学习任务来说，给定属性集，其中有些属性可能很关键、很有用，另一些属性则可能没什么用，我们将属性称为“特征”（feature），对当前学习任务有用的属性称为“相关特征”（relevant feature）、没什么用的属性称为“无关特征”（irrelavant feature）。从给定的特征集合中选择出相关特征子集的过程，称为“特征选择”（feature selection）。
【注意】：

特征的相关与无关是相对当前学习任务而言，若更换学习任务则有可能使得原本相关特征变为无关特征。例如姓名特征对于预测年龄几乎没有什么作用，但对于预测性别则有一定的参考价值。
特征选择过程必须确保不丢失重要特征，否则后续学习过程会因为重要信息的缺失而无法获得好的性能。

特征选择是一个重要的“数据预处理”过程，在现实机器学习任务中，获得数据之后通常先进行特征选择，此后再训练学习器，那么为什么要进行特征选择呢？

首先，我们在现实任务中经常会遇到维数灾难问题，这是由于特征过多而造成的，若能从中选择出重要的特征，使得后续学习过程仅需在一部分特征上构建模型，则维数灾难问题会大为减轻。从这个意义上来说，特征选 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/%E7%89%B9%E5%BE%81%E5%BD%92%E4%B8%80%E5%8C%96/" title="特征归一化"><img class="post-bg" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/ml/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E6%95%B0%E6%8D%AE%E5%BD%92%E4%B8%80%E5%8C%96%E5%AF%B9%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%94%B6%E6%95%9B%E9%80%9F%E5%BA%A6%E4%BA%A7%E7%94%9F%E7%9A%84%E5%BD%B1%E5%93%8D-2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="特征归一化"></a></div><div class="recent-post-info"><a class="article-title" href="/%E7%89%B9%E5%BE%81%E5%BD%92%E4%B8%80%E5%8C%96/" title="特征归一化">特征归一化</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2019-05-19T14:31:06.000Z" title="发表于 2019-05-19 22:31:06">2019-05-19</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">特征工程</a></span></div><div class="content">概念消除数据特征之间的量纲影响，可以将所有的特征都统一到一个大致相同的数值区间内，使得不同指标之间具有可比性。例如，分析一个人的身高和年龄对健康的影响，通常身高用 cm 作为单位，而年龄用岁作为单位，那么身高特征会大致在 160180 cm 的数值范围内，年龄特征会在 1100 岁的范围内，分析的结果显然会倾向于数值差别比较大的身高特征。想要得到更为准确的结果，就需要对数据进行特征归一化（Normalization）处理，使各指标处于同一数值量级，以便进行分析。特征归一化有时被称为特征缩放或特征规范化。
【量纲（dimension）】：物理量固有的、可度量的物理属性。例如，人的身高、体重和年龄等。
【问答 QA】：

问：为什么对拥有不同数值量级特征的数据集进行分析，其结果会倾向于数值差别较大的特征？
答：具体内容可以参考这篇博文 K-近邻算法 问题 QA 中的“为什么要做数据归一化”。

通常对于需要计算数据集特征距离的模型，我们都需要提前对数据集做特征归一化，尤其是数值型的数据。
常用方法
线性函数归一化（Min-Max Scaling）：又称为 min-max 缩放，是对原始数据 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/AGNES/" title="AGNES"><img class="post-bg" src="https://i.loli.net/2019/04/22/5cbdb4a07b184.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="AGNES"></a></div><div class="recent-post-info"><a class="article-title" href="/AGNES/" title="AGNES">AGNES</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2019-04-22T12:46:04.000Z" title="发表于 2019-04-22 20:46:04">2019-04-22</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB/">聚类</a></span></div><div class="content">AGNES（AGglomerative NESting 的简写）是一种采用自底向上聚合策略的层次聚类算法。
【工作过程】：

先将数据集中的每个样本看作一个初始聚类簇；
然后在算法运行的每一步中找出距离最近的两个聚类簇进行合并；
步骤（2）不断重复，直至达到预设的聚类簇的个数。

【关键】：如何计算聚类簇之间的距离。
实际上，每个簇是一个样本集合，因此，只需采用关于集合的某种距离即可。$$\text{最小距离：}d_{min}(C_i, C_j) &#x3D; min_{x\in C_i,z\in C_j}dist(x,z)$$
$$\text{最大距离：}d_{max}(C_i, C_j) &#x3D; max_{x\in C_i,z\in C_j}dist(x,z)$$
$$\text{平均距离：}d_{avg}(C_i, C_j) &#x3D; \frac{1}{|C_i||C_j|}\sum_{x\in C_i}\sum_{z\in C_j}dist(x,z)$$
显然，最小距离由两个簇的最近样本决定，最大距离由两个簇的最远样本决定，而平均距离则由两个簇的所有样本共同决定。
当 ...</div></div></div><div class="recent-post-item"><div class="post_cover left"><a href="/%E5%AD%A6%E4%B9%A0%E5%90%91%E9%87%8F%E9%87%8F%E5%8C%96-LVQ/" title="学习向量量化 LVQ"><img class="post-bg" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/ml/cluster/%E8%81%9A%E7%B1%BB%20cover%20%E5%9B%BE.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="学习向量量化 LVQ"></a></div><div class="recent-post-info"><a class="article-title" href="/%E5%AD%A6%E4%B9%A0%E5%90%91%E9%87%8F%E9%87%8F%E5%8C%96-LVQ/" title="学习向量量化 LVQ">学习向量量化 LVQ</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2019-04-21T03:22:56.000Z" title="发表于 2019-04-21 11:22:56">2019-04-21</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><i class="fas fa-angle-right article-meta-link"></i><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%81%9A%E7%B1%BB/">聚类</a></span></div><div class="content">学习向量量化（Learning Vector Quantization，简称 LVQ）与 K 均值算法类似，也是试图找到一组原型向量来刻画聚类结构，但与一般聚类算法不同的是，LVQ 假设数据样本带有类别标记，学习过程利用样本的这些监督信息来辅助聚类。
给定样本集 $D &#x3D; {(x_1, y_1), (x_2, y_2), \cdots, (x_n, y_n)}$，每个样本 $x_i$ 是由 m 个属性描述的特征向量 $(x_i^1, x_i^2, \cdots, x_i^m), y_i \in Y$ 是样本 $x_i$ 的类别标记。
【目标】：学得一组 m 维原型向量 ${p_1, p_2, \cdots, p_k}$，每个原型向量代表一个聚类簇，簇标记 $t_j \in Y$。
【算法描述】：

输入：样本集 $D &#x3D; {(x_1, y_1), (x_2, y_2), \cdots, (x_n, y_n)}$；原型向量个数 k，各原型向量预设的类别标记 ${t_1, t_2, \cdots, t_k}$；学习率 $\eta \in (0, 1)$。
输出：原型向量 ...</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/3/#content-inner"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/#content-inner">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/#content-inner">5</a><a class="extend next" rel="next" href="/page/5/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">clvsit</div><div class="author-info__description">人生不是戏剧，而我亦非主角</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">65</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">20</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">51</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/clvsit"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/clvsit" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">知乎和 CSDN 同名 clvsit</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/PyramidKV/" title="PyramidKV">PyramidKV</a><time datetime="2024-06-14T12:27:21.000Z" title="发表于 2024-06-14 20:27:21">2024-06-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/Long-Context-%E8%B0%83%E7%A0%94/" title="Long Context 调研">Long Context 调研</a><time datetime="2024-06-10T14:43:57.000Z" title="发表于 2024-06-10 22:43:57">2024-06-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/vocab-size-%E7%A0%94%E7%A9%B6/" title="vocab size 调研">vocab size 调研</a><time datetime="2024-05-31T15:47:07.000Z" title="发表于 2024-05-31 23:47:07">2024-05-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARCoT-Detecting-and-Rectifying-Factual-Inconsistency-in-Reasoning-by-Reversing-Chain-of-Thought/" title="论文阅读：RCoT Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought"><img src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/RCoT%EF%BC%9ADetecting%20and%20Rectifying%20Factual%20Inconsistency%20in%20Reasoning%20by%20Reversing%20Chain-of-Thought/Figure%204.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读：RCoT Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought"/></a><div class="content"><a class="title" href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARCoT-Detecting-and-Rectifying-Factual-Inconsistency-in-Reasoning-by-Reversing-Chain-of-Thought/" title="论文阅读：RCoT Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought">论文阅读：RCoT Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought</a><time datetime="2024-05-17T13:37:04.000Z" title="发表于 2024-05-17 21:37:04">2024-05-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/Transformer-%E6%9E%B6%E6%9E%84%E4%B8%AD%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/" title="Transformer 架构中的位置编码">Transformer 架构中的位置编码</a><time datetime="2024-05-16T06:56:59.000Z" title="发表于 2024-05-16 14:56:59">2024-05-16</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            <a class="card-more-btn" href="/categories/" title="查看更多">
    <i class="fas fa-angle-right"></i></a>
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/LLM/"><span class="card-category-list-name">LLM</span><span class="card-category-list-count">9</span></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/LLM/LLM-%E5%8E%8B%E7%BC%A9/"><span class="card-category-list-name">LLM 压缩</span><span class="card-category-list-count">1</span></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/LLM/LLM-%E5%8E%8B%E7%BC%A9/kv-cache-%E5%8E%8B%E7%BC%A9/"><span class="card-category-list-name">kv cache 压缩</span><span class="card-category-list-count">1</span></a></li></ul></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/LLM/SFT/"><span class="card-category-list-name">SFT</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/LLM/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/"><span class="card-category-list-name">数据增强</span><span class="card-category-list-count">2</span></a><ul class="card-category-list child"><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/LLM/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/%E6%95%B0%E6%8D%AE%E5%AD%90%E9%9B%86%E6%8C%91%E9%80%89/"><span class="card-category-list-name">数据子集挑选</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/LLM/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/%E6%95%B0%E6%8D%AE%E6%9E%84%E9%80%A0/"><span class="card-category-list-name">数据构造</span><span class="card-category-list-count">1</span></a></li></ul></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/LLM/%E6%A8%A1%E5%9E%8B%E5%89%AA%E6%9E%9D/"><span class="card-category-list-name">模型剪枝</span><span class="card-category-list-count">1</span></a></li></ul></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/%E9%87%8D%E6%8E%92/" style="font-size: 1.1em; color: #999">重排</a> <a href="/tags/vLLM-%E6%A1%86%E6%9E%B6/" style="font-size: 1.2em; color: #999da3">vLLM 框架</a> <a href="/tags/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/" style="font-size: 1.3em; color: #99a1ac">位置编码</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E6%9E%84%E9%80%A0/" style="font-size: 1.1em; color: #999">数据构造</a> <a href="/tags/%E7%A1%AC%E4%BB%B6/" style="font-size: 1.2em; color: #999da3">硬件</a> <a href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" style="font-size: 1.5em; color: #99a9bf">论文阅读</a> <a href="/tags/kv-cache-%E5%8E%8B%E7%BC%A9/" style="font-size: 1.1em; color: #999">kv cache 压缩</a> <a href="/tags/%E5%AD%90%E8%AF%8D%E6%A8%A1%E5%9E%8B/" style="font-size: 1.1em; color: #999">子词模型</a> <a href="/tags/prompt-%E5%B7%A5%E7%A8%8B/" style="font-size: 1.2em; color: #999da3">prompt 工程</a> <a href="/tags/%E6%B7%B7%E5%90%88%E6%A3%80%E7%B4%A2/" style="font-size: 1.1em; color: #999">混合检索</a> <a href="/tags/%E6%9F%A5%E8%AF%A2%E5%8F%98%E6%8D%A2/" style="font-size: 1.1em; color: #999">查询变换</a> <a href="/tags/%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB/" style="font-size: 1.1em; color: #999">层次聚类</a> <a href="/tags/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/" style="font-size: 1.1em; color: #999">模型部署</a> <a href="/tags/RAG/" style="font-size: 1.4em; color: #99a5b6">RAG</a> <a href="/tags/%E5%B7%A5%E4%BD%9C%E5%86%85%E5%AE%B9/" style="font-size: 1.2em; color: #999da3">工作内容</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83/" style="font-size: 1.1em; color: #999">分布式并行训练</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%AD%90%E9%9B%86%E6%8C%91%E9%80%89/" style="font-size: 1.1em; color: #999">数据子集挑选</a> <a href="/tags/%E5%AD%A6%E4%B9%A0/" style="font-size: 1.1em; color: #999">学习</a> <a href="/tags/%E6%8F%90%E5%89%8D%E9%80%80%E5%87%BA/" style="font-size: 1.2em; color: #999da3">提前退出</a> <a href="/tags/%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA/" style="font-size: 1.1em; color: #999">检索增强</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>归档</span><a class="card-more-btn" href="/archives/" title="查看更多">
    <i class="fas fa-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/06/"><span class="card-archive-list-date">六月 2024</span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/05/"><span class="card-archive-list-date">五月 2024</span><span class="card-archive-list-count">9</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/04/"><span class="card-archive-list-date">四月 2024</span><span class="card-archive-list-count">9</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/03/"><span class="card-archive-list-date">三月 2024</span><span class="card-archive-list-count">5</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/02/"><span class="card-archive-list-date">二月 2024</span><span class="card-archive-list-count">5</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/01/"><span class="card-archive-list-date">一月 2024</span><span class="card-archive-list-count">3</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/12/"><span class="card-archive-list-date">十二月 2023</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/11/"><span class="card-archive-list-date">十一月 2023</span><span class="card-archive-list-count">1</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">65</div></div><div class="webinfo-item"><div class="item-name">本站总字数 :</div><div class="item-count">161.6k</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2024-06-26T16:05:20.117Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/banner.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By clvsit</div><div class="footer_custom_text">城南芳草城北溪，池塘烟柳归鸟栖。忽听窗外风吹雨，一梦花落月近西。</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>window.typedJSFn = {
  init: (str) => {
    window.typed = new Typed('#subtitle', Object.assign({
      strings: str,
      startDelay: 300,
      typeSpeed: 150,
      loop: true,
      backSpeed: 50,
    }, null))
  },
  run: (subtitleType) => {
    if (true) {
      if (typeof Typed === 'function') {
        subtitleType()
      } else {
        getScript('https://cdn.jsdelivr.net/npm/typed.js@2.1.0/dist/typed.umd.min.js').then(subtitleType)
      }
    } else {
      subtitleType()
    }
  }
}
</script><script>function subtitleType () {
  if (true) {
    typedJSFn.init("sadasd")
  } else {
    document.getElementById("subtitle").textContent = "s"
  }
}
typedJSFn.run(subtitleType)</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"]):not([href="/music/"]):not([href="/no-pjax/"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener
  btf.removeGlobalFnEvent('pjax')
  btf.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>