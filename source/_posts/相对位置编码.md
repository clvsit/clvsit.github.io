---
title: 相对位置编码
date: 2024-04-09 23:40:29
mathjax: true
tags:
category:
- 深度学习
- 位置编码
---

绝对位置编码计算 Attention 矩阵，m 和 n 分别表示两个位置。

$$
q_m = W_q(x_m + p_m) \quad k_n = W_k(x_n + p_n) \tag{1}
$$

$$
q_mk_n^T = W_q x_m x_n^T W_k^T + W_q x_m p_n^T W_k^T + W_q p_m x_n^T W_k^T + W_q p_m p_n^T W_k^T \tag{2}
$$

可以看到公式（2）中，第一项与位置信息无关，第二项至第四项和位置信息相关。因此，研究者通常是直接修改第二项至第四项的内容，直接在 attention 矩阵中添加相对位置信息。常见的有以下几种方式：

- **XLNET 式**：将第二项至第四项都做了改变，将$p_n$替换成 sinusoidal 生成式编码$\hat{R}_{n-m}$，将$p_m$换成两个可以训练的向量 u 和 v。

$$
q_mk_n^T = W_q x_m x_n^T W_k^T + W_q x_m \hat{R}_{n-m}^T W_k^T + W_q u x_n^T W_k^T + W_q v \hat{R}_{n-m}^T W_k^T \tag{3}
$$

- **T5 式**：这篇论文的作者认为输入和位置间不应过多的交互，因此将第二项和第三项删除，将第四项替换为一个可学习的偏置$b_{m, n}$，在 attention 矩阵的基础上加一个可训练的偏置项。

$$
q_m k_n^T = W_q x_m x_n^T W_k^T + b_{m, n} \tag{4}
$$

- **DeBerta 式**：和 T5 的构造相反，它舍弃了第四项，保留了第二项和第三项，并将位置信息替换成了相对位置编码$\hat{R}_{n-m}$。

$$
q_mk_n^T = W_q x_m x_n^T W_k^T + W_q x_m \hat{R}_{n-m}^T W_k^T + W_q \hat{R}_{n-m} x_n^T W_k^T \tag{5}
$$

Attention 的核心运算是内积，所以我们希望经过内积的结果能够带有相对信息。也就是说$q_m$和$k_n$的内积仅与输入$x_m$、$x_n$和它们的相对位置 m - n 有关，那么我们可以假设存在函数 g，使得：

$$
q_m k_n^T = g(x_m, x_n, m - n)
$$


[RoPE](https://www.wolai.com/eY9vUUBq5wpFJnwcXHja99)

[RoPE + 位置线性内插](https://www.wolai.com/gzj8R1Q2TLTPJNMaDjXvst)

[NTK-Aware Scaled RoPE](https://www.wolai.com/ecZCpeKEbgs642QHrBSvx8)

[Dynamic NTK RoPE](https://www.wolai.com/cFAwaDwcm51jVkW5bZmWqM)



# QA 部分

**问题**：为什么相对位置编码具有外推性？

相对位置编码具有外推性（extrapolation ability）的原因在于它**不是将位置信息编码为与序列中的绝对位置直接相关的固定模式，而是将注意力权重或其他模型组件与元素之间的相对距离联系起来**。这样，即使在训练时未见过的更长的序列中，模型仍然能够根据学到的相对位置模式进行推断。以下是相对位置编码具有外推性的一些原因：

1. **相对距离的一致性**：在相对位置编码中，相对距离是一致的，无论元素在序列中的绝对位置如何。因此，模型可以学习到“两个单词相隔一个位置”这样的模式，并将其应用于序列的任何部分。
2. **长度不变性**：由于相对位置编码关注的是相对距离而非绝对位置，因此它天然不受输入序列长度的影响。这使得模型能够处理比训练时见过的序列更长的序列，而不会丧失对位置关系的理解。
3. **泛化能力**：相对位置编码使得模型更容易泛化到不同的上下文中，因为它依赖于元素之间的相对关系，而这些关系在不同的文本中是普遍存在的。
4. **动态编码**：在某些实现中，相对位置信息是动态计算的，而不是像传统的绝对位置编码那样是固定的。这意味着模型可以根据需要为任何给定的序列位置对生成编码，从而提供更大的灵活性和外推能力。
5. **适应性强**：相对位置编码通常使模型能够更好地适应序列操作，如切割、拼接或排序，因为这些操作不会影响元素之间的相对位置关系。

---


