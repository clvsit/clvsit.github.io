---
title: 特征选择-包裹式选择
date: 2019-05-28 21:18:00
top_img: https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/ml/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/feature%20selection/%E5%8C%85%E8%A3%B9%E5%BC%8F%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9.png
cover: https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/ml/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/feature%20selection/%E5%8C%85%E8%A3%B9%E5%BC%8F%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9.png
mathjax: true
tags:
category:
- 机器学习
---

包裹式选择与过滤式选择不考虑后续学习器不同，直接把最终使用的学习器的性能作为特征子集的评价准则。换言之，包裹式选择的目的就是为给定学习器选择最有利于其性能、“量身定做”的特征子集。

【与过滤式选择的区别】：
- 包裹式选择方法直接针对给定学习器进行优化，因此，从最终学习器性能来看，包裹式选择比过滤式选择更好；
- 但另一方面，由于在特征选择过程中需多次训练学习器，因此包裹式选择的计算开销通常比过滤式选择大得多。

![特征选择-包裹式选择思维导图.png](https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/ml/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/feature%20selection/%E5%8C%85%E8%A3%B9%E5%BC%8F%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9.png)

## 递归特征消除
递归特征消除（Recursive Feature Elimination）使用一个基模型（学习器）来进行多轮训练，每轮训练后移除若干特征，再基于新的特征集进行下一轮训练。

【sklearn 官方解释】：对特征含有权重的预测模型，RFE 通过递归减少待考察特征集规模来选择特征。
1. 首先，预测模型在原始特征集上进行训练，通过 coef\_ 属性或 feature\_importances\_ 属性为每个特征指定一个权重；
2. 然后，剔除那些权重绝对值较小的特征；
3. 如此循环，直到剩余的特征数量达到所需的特征数量。

需要注意的是，RFE 的稳定性很大程度上取决于迭代时，底层使用的预测模型。如果 RFE 采用的是普通的逻辑回归，没有经过正则化的回归是不稳定的，因此 RFE 也不稳定。若采用的是脊回归 Ridge 或 Lasso，则 RFE 稳定。

关于 RFE 的具体介绍可参考 sklearn 的 RFE [传送门](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE)

【代码实现】：回归问题
```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import Lasso


# 引入数据集
dataset_boston = load_boston()
data_boston = dataset_boston.data
target_boston = dataset_boston.target

rfe = RFE(estimator=Lasso(), n_features_to_select=4)
rfe.fit(data_boston, target_boston)
print(rfe.support_)
# 输出
array([False, False, False, False, False,  True, False,  True, False,
       False,  True, False,  True])
```

【代码实现】：分类问题
```python
from sklearn.feature_selection import RFE


# 引入数据集
dataset_iris = load_iris()
data_iris = dataset_iris.data
target_iris = dataset_iris.target

rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=2)
rfe.fit(data_iris, target_iris)
print(rfe.support_)
array([False, False,  True,  True])
```
sklearn 还提供 RFECV 方法，该方法通过交叉验证的方式执行 RFE，以此来选择最佳数量的特征：对于一个数量为 d 的特征集合，它的所有子集的个数是 $2^d-1$。例如 d = 3 时，子集个数为 $2^3-1=7$。举个例子，特征集为 {A, B, C}，那么其所有特征子集为 {A}、{B}、{C}、{A, B}、{A, C}、{B, C}、{A, B, C}。

RFE 找出所有的特征子集后，分别计算所有特征子集的验证误差，选择误差最小的特征子集作为挑选的特征。

【代码实现】：
```python
from sklearn.feature_selection import RFECV


rfecv = RFECV(estimator=DecisionTreeClassifier())
rfecv.fit(data_iris, target_iris)
print(rfecv.support_)
# 输出
array([False, False,  True,  True])
```

所有相关代码可从 [传送门](https://github.com/clvsit/Machine-Learning-Note/blob/master/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/wrapper.py) 中获得。

## LVW（Las Vegas Wrapper）
LVW 是一个典型的包裹式特征选择方法，它在拉斯维加斯（Las Vegas method）框架下使用随机策略来进行子集搜索，并以最终分类器的误差为特征子集评价准则。

【算法】：
- 输入：数据集 D；特征集 A；学习算法 $\varSigma$；停止条件控制参数 T。
- 输出：特征子集 A*。
- 过程：
1. 初始化误差 E 为正无穷，d = |A|，A* = A，t = 0；
2. 进入循环，循环停止条件为 while t < T；
3. 随机产生特征子集 A'，设置 d' = |A'|；
4. 选择特征子集对应部分的数据集 $D^{A'}$，使用交叉验证法来估计学习器 $\varSigma$ 的误差。误差是特征子集 A' 上的误差，若它比当前特征子集 A 上的误差更小，或误差相当但 A' 中包含的特征数更少，则执行（a），否则执行（b）。
    - （a）：t = 0，E = E'，d = d'，A* = A'；
    - （b）：t = t + 1
5. 输出特征子集 A*。

【注意】：由于 LVW 算法中特征子集搜索采用了随机策略，而每次特征子集评价都需要训练学习器，计算开销很大，因此算法设置了停止条件控制参数 T。然而，整个 LVW 算法是基于拉斯维加斯方法框架，若初始特征数很多（即 |A| 很大）、T 设置较大，则算法可能运行很长时间都达不到停止条件。换言之，若有运行时间限制，则有可能给不出解。

另外还有一个经典的算法——蒙特卡罗方法。这两个以著名赌城名字命名的随机化方法的主要区别是：若有时间限制，则拉斯维加斯方法或者给出满足要求的解，或者不给出解；而蒙特卡罗方法一定会给出解，虽然给出的解未必满足要求；若无时间限制，则两者都能给出满足要求的解。

## 参考
- 《机器学习》周志华
- 《百面机器学习》
- 递归特征消除（RFE）+ 交叉验证：https://www.jianshu.com/p/025395835591
- 特征选择 (feature_selection)：https://www.cnblogs.com/stevenlk/p/6543628.html#%E7%A7%BB%E9%99%A4%E4%BD%8E%E6%96%B9%E5%B7%AE%E7%9A%84%E7%89%B9%E5%BE%81-removing-features-with-low-variance