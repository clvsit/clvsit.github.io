---
title: vocab size 调研
date: 2024-05-31 23:47:07
tags:
category:
- NLP
---

在 Meta 开源 Llama 模型后，大量的微调模型出现，大部分模型都在 Llama 模型的基础上扩大了词表。例如，01-ai 的 Yi-1.5 将词表大小从 32000 扩展至 64000。

**扩大词表的作用**：

- **提高语言覆盖率**：扩大词表可以使模型覆盖更多的词汇，特别是包含罕见词汇、专业术语、新词以及多语言环境下的词汇。这对于处理多样化的文本数据，提高模型在特定领域（如医学、法律或多语言环境）的应用效果非常重要。  
- **减少未知词（OOV）问题**：通过增加词表大小，可以减少在文本处理中遇到的未知词（Out-Of-Vocabulary, OOV）问题。这有助于提高模型处理未见过文本的能力，从而在一定程度上提升模型的泛化性能。  
- **提升模型细粒度**：较大的词表使得模型能够识别和生成更加细粒度的文本信息。例如，在生成任务中，模型能够生成更丰富、更精确的文本内容；在理解任务中，模型能更准确地捕捉到文本的细微差别。  
- **增强多语言和跨文化理解**：对于支持多种语言的大型语言模型，扩大词表是必要的，以确保覆盖更广泛的语言和文化。这有助于模型在全球范围内的适用性和灵活性，提升跨语言、跨文化交流的能力。  
- **改善特定任务的性能**：对于一些需要高度专业化词汇或非常具体语言表达的任务（如翻译、情感分析、实体识别等），扩大词表能够直接提升模型在这些任务上的性能。

**扩大词表带来的挑战**：

- **增加计算复杂度和参数量**：词表的扩大直接增加了模型参数的数量，尤其是在处理输入和输出层时，这会导致模型训练和推理时的计算复杂度显著增加，需要更多的计算资源和时间。同时也需要更多的内存来存储额外的参数。这不仅增加了存储成本，也可能限制了模型在资源受限的设备上的部署和应用。  
- **稀疏性问题**：随着词表的增大，词汇和实体的分布可能变得更加稀疏，特别是对于那些出现频率较低的词汇。稀疏性问题会影响模型对这些词汇的学习效果，导致模型在处理罕见词汇时性能下降。
- **过拟合风险**：更大的词表可能会增加模型的过拟合风险。模型可能会在训练数据上学到过多的细节，而无法泛化到未见过的数据上。这种情况在数据量相对较小的情况下尤其明显。
- **词汇冗余和歧义**：不加选择地扩大词表可能会引入词汇冗余和增加歧义，尤其是当引入的新词汇在不同上下文中有不同含义时。这可能会使模型在理解和生成文本时产生混淆，影响最终的性能。

使用 BPE 算法可以在一定程度上平衡词表大小和词汇覆盖范围之间的关系，可以缓解稀疏性问题和减少模型复杂度（计算以及内存需求）。那么，有哪些方法可以判断词表内词汇是否冗余和歧义？

- **采用子词分割技术**：**Byte Pair Encoding (BPE)**、**SentencePiece**、**WordPiece** 等子词分割技术可以有效减少词汇的冗余。通过将罕见词汇分解为更小的可重用单元（如子词或字符），这些技术可以在减少词表大小的同时保持较高的语言覆盖率，同时也减轻了词汇歧义的问题。  
- **词义消岐技术**：在构建词表时，使用词义消岐技术识别多义词的不同含义，并将其视为不同的词汇。这有助于减少模型在处理这些词汇时的歧义。词义消岐可以基于上下文、词典定义或是通过半监督学习和无监督学习方法来实现。  
- **词表清洗和规范化**：对词表进行预处理，包括去除停用词、词形还原、词干提取等，可以有效减少冗余。此外，对词汇进行规范化处理（如统一同义词、处理不同的拼写变体等），可以进一步减少冗余和歧义。  
- **利用外部知识库**：结合外部知识库（如WordNet、ConceptNet等）可以帮助识别和区分词汇的不同含义，减少歧义。通过链接到这些知识库中的同义词集（synsets）或概念，可以改善词汇的语义表示。  
- **语义聚类**：使用基于语义的聚类技术将词汇聚类，可以帮助识别冗余和歧义词汇。通过分析词汇的上下文使用情况，可以将语义相近的词汇聚合在一起，并识别出具有多个不同含义的词汇。  
- **动态词表管理**：对于一些高级的模型，可以采用动态词表管理策略，根据任务需求和上下文动态调整词表的内容。通过在线学习或实时更新词表，可以灵活应对语言的变化和特定领域的需求，减少冗余和歧义。  
- **融合上下文信息**：在模型训练和推理阶段充分利用上下文信息，可以帮助模型更好地理解和区分多义词。通过上下文信息，模型可以学习到每个词汇在特定语境中的具体含义，从而减轻歧义问题。

博客 [词表的选择如何影响语言模型训练？这可能是目前见过最好的词表选择研究](https://www.jiqizhixin.com/articles/2023-10-04-5) 提到：
- 最佳词表规模为 32000（Llama 系列模型的词表大小），词表越简单，模型收敛得越快，但后期不一定会产生更好的结果（后续出的模型词表越来越大，例如 Llama3-8B 的词表大小达到 128256，该结论可能已经“落伍”）。
- 字词比（每个 token 对应的平均字符数）增加，不会单独对模型质量产生负面影响。
