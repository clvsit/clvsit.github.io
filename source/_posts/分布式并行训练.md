---
title: 分布式并行训练
date: 2024-03-07 22:14:41
tags:
- 分布式并行训练
category:
- 模型训练
- 分布式并行训练
---

先前做 NLP 相关的工作（2023 年前），主要用到的是数据并行，例如 PyTorch 的 DP 和 DDP，对 3D 略有耳闻，没有系统地学习。在从事 LLM 相关的工作，不得不对此有所了解，无论是训练还是推理，对占用的显存有个大致的估算。

虽然在个人的笔记中记录了不少相关的内容，但本体是 吃果冻不吐果冻皮写的知乎文章[《大模型分布式训练并行技术》](https://zhuanlan.zhihu.com/p/667051845)。我所做的无非是将其整理摘录，然后对部分不理解的地方做了一个 QA（搜索资料、或者询问 GPT-4o）。如果将其当做自己的博客，就涉及到抄袭和狗尾续貂了，所以还是直接看原文吧。

在此主要补充一些参考资料，以及后续的相关工作。

# 参考资料
- DeepSpeed之ZeRO系列：将显存优化进行到底 - basicv8vc的文章 - 知乎
https://zhuanlan.zhihu.com/p/513571706
- 数据并行Deep-dive: 从DP 到 Fully Sharded Data Parallel （FSDP）完全分片数据并行 - YuxiangJohn的文章 - 知乎
https://zhuanlan.zhihu.com/p/485208899
