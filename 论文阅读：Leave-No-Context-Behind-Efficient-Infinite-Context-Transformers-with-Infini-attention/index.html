<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>论文阅读：Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention | clvsit 个人博客</title><meta name="author" content="clvsit"><meta name="copyright" content="clvsit"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="这项研究介绍了一种高效的方法，可将基于 Transformer 的大型语言模型（LLM）扩展到无限长的输入，同时限制内存和计算量。该方法的一个关键组成部分是一种新的注意力技术，被称为 Infini-attention。Infini-attention 在 vanilla 注意力机制中加入了压缩内存，并在单个 Transformer 块中建立了掩码局部注意和长期线性注意机制。 作者使用 1B 和 8">
<meta property="og:type" content="article">
<meta property="og:title" content="论文阅读：Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention">
<meta property="og:url" content="https://clvsit.github.io/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ALeave-No-Context-Behind-Efficient-Infinite-Context-Transformers-with-Infini-attention/">
<meta property="og:site_name" content="clvsit 个人博客">
<meta property="og:description" content="这项研究介绍了一种高效的方法，可将基于 Transformer 的大型语言模型（LLM）扩展到无限长的输入，同时限制内存和计算量。该方法的一个关键组成部分是一种新的注意力技术，被称为 Infini-attention。Infini-attention 在 vanilla 注意力机制中加入了压缩内存，并在单个 Transformer 块中建立了掩码局部注意和长期线性注意机制。 作者使用 1B 和 8">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Leave%20No%20Context%20Behind%20Efficient%20Infinite%20Context%20Transformers%20with%20Infini-attention/Figure%202.png">
<meta property="article:published_time" content="2024-04-30T14:57:25.000Z">
<meta property="article:modified_time" content="2024-06-29T15:00:45.489Z">
<meta property="article:author" content="clvsit">
<meta property="article:tag" content="long context">
<meta property="article:tag" content="论文阅读">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Leave%20No%20Context%20Behind%20Efficient%20Infinite%20Context%20Transformers%20with%20Infini-attention/Figure%202.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://clvsit.github.io/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ALeave-No-Context-Behind-Efficient-Infinite-Context-Transformers-with-Infini-attention/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '论文阅读：Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-06-29 23:00:45'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.1.1"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">74</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">29</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">59</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Leave%20No%20Context%20Behind%20Efficient%20Infinite%20Context%20Transformers%20with%20Infini-attention/Figure%202.png')"><nav id="nav"><span id="blog-info"><a href="/" title="clvsit 个人博客"><span class="site-name">clvsit 个人博客</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">论文阅读：Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-04-30T14:57:25.000Z" title="发表于 2024-04-30 22:57:25">2024-04-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-06-29T15:00:45.489Z" title="更新于 2024-06-29 23:00:45">2024-06-29</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/">研究方向</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/long-context/">long context</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>13分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="论文阅读：Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>这项研究介绍了一种高效的方法，可将基于 Transformer 的大型语言模型（LLM）扩展到无限长的输入，同时限制内存和计算量。该方法的一个关键组成部分是一种新的注意力技术，被称为 Infini-attention。Infini-attention 在 vanilla 注意力机制中加入了压缩内存，并在单个 Transformer 块中建立了掩码局部注意和长期线性注意机制。</p>
<p>作者使用 1B 和 8B LLM，在长上下文语言建模基准、1M 序列长度的 passkey context block 检索和 500K 长度的书籍摘要任务中展示了该方法的有效性。该方法引入了最小的有界内存参数，实现了 LLM 的快速流推理。</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>图 2 比较了提出的模型、Infini-Transformer 和 Transformer-XL。与 Transformer-XL 类似，Infini-Transformer 也是在一个片段序列上运行。作者在每个片段中计算标准的因果点积注意力上下文。因此，点积注意力计算是局部的，即它覆盖了当前片段中索引为 S 的 N 个 token（N 为片段长度）。然而，局部注意力（Dai 等人，2019 年）在处理下一个片段时会丢弃上一个片段的注意力状态。</p>
<p><img src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Leave%20No%20Context%20Behind%20Efficient%20Infinite%20Context%20Transformers%20with%20Infini-attention/Figure%202.png"></p>
<blockquote>
<p>图 2：Infini-Transformer（上图）拥有完整的上下文历史记录，而 Transformer-XL（下图）则会丢弃旧的上下文，因为它只缓存最后一个片段的 KV 状态。</p>
</blockquote>
<p>在 Infini-Transformers中，作者并没有丢弃旧的KV注意力状态，而是建议重新使用它们，用压缩记忆来保持整个上下文历史。因此，Infini-Transformers 的每个注意力层都具有全局压缩状态和局部细粒度状态。作者将这种高效的注意力机制称为 Infini-attention，图 1 是其示意图，下文将对其进行正式描述。</p>
<p><img src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Leave%20No%20Context%20Behind%20Efficient%20Infinite%20Context%20Transformers%20with%20Infini-attention/Figure%201.png"></p>
<blockquote>
<p>图 1：无限注意力有一个额外的线性注意压缩存储器，用于处理无限长的上下文。${KV}_{s-1}$ 和 ${KV}_s$ 分别是之前和当前输入片段的注意力键值，Q 是注意力查询。PE 表示位置嵌入。</p>
</blockquote>
<h2 id="Infini-attention"><a href="#Infini-attention" class="headerlink" title="Infini-attention"></a>Infini-attention</h2><p>如图 1 所示，Infini-attention（无限注意力）计算本地和全局上下文状态，并将它们结合起来进行输出。与多头注意力（MHA）类似，除了点积注意力外，它还为每个注意力层保留 H 个并行压缩存储器（H 为注意力头数）。</p>
<h3 id="Scaled-Dot-product-Attention"><a href="#Scaled-Dot-product-Attention" class="headerlink" title="Scaled Dot-product Attention"></a>Scaled Dot-product Attention</h3><p>multi-head scaled 点积注意力，特别是其自我注意力变体，一直是 LLM 的主要构建模块。在自回归生成模型中，MHA 的强大建模能力和时间掩蔽的便利性得到了广泛使用。</p>
<p>Vanilla MHA 中的单个头根据输入片段序列 $X \in \R^{N \times d_{value}}$ 计算其注意力上下文 $A_{dot} \in \R^{N \times d_{value}}$。首先，计算注意力查询、键和值的状态：</p>
<p>$$<br>K &#x3D; XW_K, \quad V &#x3D; XW_v \quad and \quad Q &#x3D; XW_Q. \tag{1}<br>$$</p>
<p>在这，$W_K \in \R^{d_{model} \times d_{key}}, W_V \in \R^{d_{model} \times d_{value}}, and \ W_Q \in \R^{d_{model} \times d_{key}}$ 是可训练的投影矩阵。然后，将注意力上下文计算为所有其他值的加权平均值，即：</p>
<p>$$<br>A_{dot} &#x3D; softmax(\frac{QK^T}{\sqrt{d_{model}}})V. \tag{2}<br>$$</p>
<p>就 MHA 而言，为每个序列元素并行计算 H 个注意力上下文向量，然后将它们沿第二维度串联起来，最后将串联向量投射到模型空间，从而获得注意力输出。</p>
<h3 id="压缩记忆"><a href="#压缩记忆" class="headerlink" title="压缩记忆"></a>压缩记忆</h3><p>在 Infini-attention 中，不再为压缩内存计算新的内存条目，而是重复使用点积注意力计算中的查询、键和值状态（Q、K 和 V）。点积注意力和压缩内存之间的状态共享和重用不仅能实现高效的即插即用式长上下文适应，还能加快训练和推理速度。与之前的工作（Munkhdalai 等人，2019 年）类似，目标是在压缩存储器中存储键和值状态的绑定，并通过查询向量进行检索。</p>
<p>虽然文献中提出了不同形式的压缩记忆，但为了简化和提高计算效率，在本研究中，用关联矩阵对记忆进行参数化。通过这种方法，还可以将记忆更新和检索过程视为线性注意力机制，并利用相关方法中的稳定训练技术。特别值得一提的是，作者采用了 Katharopoulos 等人（2020 年）的更新规则和检索机制，这主要是由于其简单性和具有竞争力的性能。</p>
<p><strong>记忆检索</strong>：在 Infini-attention 中，通过查询 $Q \in \R^{N \times d_{key}}$ 从存储器 $M_{s-1} \in \R^{d_{key} \times d_{value}}$ 中检索出新的内容 $A_{mem} \in \R^{N \times d_{value}}$：</p>
<p>$$<br>A_{mem} &#x3D; \frac{\sigma(Q)M_{s-1}}{\sigma(Q)Z_{s-1}}. \tag{3}<br>$$</p>
<p>这里，$\sigma$ 和 $Z_{s-1} \in \R^{d_{key}}$ 分别是一个非线性激活函数和一个归一化项。由于非线性和规范方法的选择对训练的稳定性至关重要，因此按照 Katharopoulos 等人的做法，记录所有 keys 的总和作为归一化项 $Z_{s-1}$，并使用 element-wise ELU + 1 作为激活函数。</p>
<p><strong>记忆更新</strong>：检索完成后，用新的 KV 实体更新记忆和归一化项，得到下一个状态，即：</p>
<p>$$<br>M_s \leftarrow M_{s - 1} + \sigma(K)^TV \quad and \quad Z_s \leftarrow Z_{s - 1} + \sum_{t&#x3D;1}^N \sigma(K_t). \tag{4}<br>$$</p>
<p>然后，新的记忆状态 $M_s$ 和 $Z_s$ 被传递到下一个片段 S+1，在每个注意力层中新城递归。式（4）中的右侧项 $\sigma(K)^TV$ 被称为关联绑定算子（associative bindling operator）。</p>
<p>受 delta 规则成功的启发，作者也将其纳入了 Infini-attention。delta 规则首先检索现有的 value 实体并将其从新 values 中减去，然后再应用用关联绑定作为新的更新，从而尝试略微改进的记忆更新。</p>
<p>$$<br>M_s \leftarrow M_{s - 1} + \sigma(K)^T(V - \frac{\sigma(K)M_{s-1}}{\sigma(K)Z_{s-1}}). \tag{5}<br>$$</p>
<p>如果 KV 绑定已经存在于记忆中，这种更新规则（线性 + delta）将不对关联矩阵进行修改，同时仍会跟踪与前一种更新规则（线性）相同的归一化项，以确保数值稳定性。</p>
<p><strong>长期上下文注入</strong>：通过一个学习到的门控标量 $\beta$ 来聚合 local 注意力状态 $A_{dot}$ 和记忆检索内容 $A_{mem}$：</p>
<p>$$<br>A &#x3D; sigmoid(\beta) \odot A_{mem} + (1 - sigmoid(\beta)) \odot A_{dot}. \tag{6}<br>$$</p>
<p>这样，每个头只需增加一个标量值作为训练参数，同时还能在模型中的长期信息流和局部信息流之间进行可学习的权衡。</p>
<p>与标准 MHA 类似，对于多头 Infini-attention，并行计算 H 个上下文状态，并将其串联和投影，得到最终的注意力输出 $O \in \R^{N \times d_{model}}$：</p>
<p>$$<br>O &#x3D; [A^1; \ldots A^H]W_O \tag{7}<br>$$</p>
<p>其中，$W_O \in \R^{H \times d_{value} \times d_{model}}$ 是可训练权重。</p>
<h2 id="内存和有效上下文窗口"><a href="#内存和有效上下文窗口" class="headerlink" title="内存和有效上下文窗口"></a>内存和有效上下文窗口</h2><p>Infini-Transformer 可以实现无限制的上下文窗口和有限制的内存占用。为了说明这一点，表 1 列出了以前的 segment-level 内存模型，以及根据模型参数和输入段长度定义的上下文内存占用空间和有效上下文长度。Infini-Transformer 在单层中将压缩上下文存储在 $M_s$ 和每个头的 $Z_s$ 中时，内存复杂度为 $d_{key} \times d_{value} + d_{key}$，而对于其他模型，复杂度随着序列维度的增加而增加——对于 Transformer-XL、Compressive Transformer 和 Memorizing Transformers 来说，内存复杂度取决于缓存大小，而对于 RTM 和 AutoCompressors 来说，内存复杂度取决于软提示大小。</p>
<p>除当前状态外，Transformer-XL 还会对上一片段缓存的 KV 状态计算注意力。Compressive Transformer 为 Transformer-XL 增加了第二个缓存，并存储过去片段激活的压缩表征。因此，它将 Transformer-XL 的上下文窗口扩展了 c × r × l，但上下文内存复杂度仍然很大。</p>
<p>在此基础上，Memorizing Transformers 选择存储整个 KV 状态作为输入序列的上下文。由于在这种情况下存储成本过高，他们将上下文计算限制在单层。通过利用快速 kNN 检索器，Memorizing Transformers 可以建立一个覆盖整个序列历史的上下文窗口（长度为 N × S），但存储成本会增加。实验表明，在 Memorizing Transformers 的基础上，Infini-Transformer LM 可以达到 100 倍以上的压缩率，同时还能进一步提高困惑度得分。</p>
<p>RMT 和 AutoCompressors 允许潜在的无限上下文长度，因为它们会将输入压缩成摘要向量，然后将其作为额外的软提示输入传递给后续片段。但实际上，这些技术的成功与否在很大程度上取决于软提示向量的大小。也就是说，有必要增加软提示（摘要）向量的数量，以提高自动压缩器的性能（Chevalier et al. 在 AutoCompressors 中还观察到（Chevalier 等人，2023 年），训练此类提示压缩技术需要一个高效的压缩目标（Ge 等人，2023 年）。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>在涉及超长输入序列的基准测试中对 Infini-Transformer 模型进行了评估：长上下文语言建模、100 万长度的 passkey 上下文块检索和 50 万长度的图书摘要任务。对于语言建模基准，从头开始训练模型，而对于 passkey 和书籍摘要任务，不断预训练现有的 LLM，以突出该方法的即插即用长上下文适应能力。</p>
<h2 id="长上下文语言建模"><a href="#长上下文语言建模" class="headerlink" title="长上下文语言建模"></a>长上下文语言建模</h2><p>作者在 PG19（Rae 等人，2019 年）和 Arxiv-math（Wu 等人，2022 年）基准上训练和评估了小型 Infini-Transformer 模型。设置与 Memorizing Transformers（Wu 等人，2022 年）的设置非常相似。也就是说，所有模型都有 12 层、8 个注意头，每个注意头的维度为 128，FFN 的隐藏层为 4096。</p>
<p>作者将所有注意力层的 Infini-attention 段长度 N 设为 2048，将输入序列长度设为 32768，以进行训练。这样，Infini-attention 就能在压缩内存状态下展开 16 个 steps。对于 RMT 基线，在摘要提示长度为 50、100 和 150，序列长度为 4096、8196 和 32768 的情况下进行了多次运行。在 8196 长度的序列上训练时，使用 100 个摘要向量的 RMT 得到了最佳结果。</p>
<p>表 2 总结了语言建模实验的主要结果。Infini-Transformer 性能优于 Transformer-XL 和 Memorizing Transformers 基线，同时与 Memorizing Transformer 模型相比，其第 9 层基于向量检索的 KV 内存长度为 65K，内存参数减少了 114 倍。</p>
<p><img src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Leave%20No%20Context%20Behind%20Efficient%20Infinite%20Context%20Transformers%20with%20Infini-attention/Table%202.png"></p>
<blockquote>
<p>表 2：长上下文语言建模结果与平均 token 级别 PPL 的比较。Comp. 表示压缩率。Infini-Transformer 优于内存长度为 65K 的 memorizing transformers，压缩率达到 114 倍。</p>
</blockquote>
<p><strong>100K 长度训练</strong>：仅有一步将训练序列长度从 32K 增加到 100K，并在 Arxiv-math 数据集上训练模型。100K 训练进一步降低了 Linear 模型和 Linear + Delta 模型的 PPL 分数，分别为 2.21 和 2.20。</p>
<p><strong>门控得分可视化</strong>：图 3 展示了各层所有注意力头压缩记忆的门控得分$sigmoid(\beta)$。经过训练后，Infini-attention 中出现了两类注意力头：门控得分接近 0 或 1 的专门注意力头和得分接近 0.5 的混合注意力头。专用头要么通过局部注意力计算处理上下文信息，要么从压缩记忆中检索信息，而混合头则将当前上下文信息和长期记忆内容汇总为单一输出。有趣的是，每一层都至少有一个 short-range 头，允许输入信号向前传播，直到输出层。作者还观察到，在整个前向计算过程中，长短期内容检索交错进行。</p>
<p><img src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Leave%20No%20Context%20Behind%20Efficient%20Infinite%20Context%20Transformers%20with%20Infini-attention/Figure%203.png"></p>
<blockquote>
<p>图 3：经过训练后，Infini-attention 中出现了两类头：门控得分接近 0 或 1 的专门头和得分接近 0.5 的混合头。专门头要么通过局部注意机制处理上下文信息，要么从压缩记忆中检索，而混合头则将当前上下文信息和长期记忆内容聚合在一起，形成单一输出。</p>
</blockquote>
<h2 id="LLM-持续预训练"><a href="#LLM-持续预训练" class="headerlink" title="LLM 持续预训练"></a>LLM 持续预训练</h2><p>作者为现有 LLM 的长上下文适应进行了轻量级持续预训练。预训练数据包括 PG19 和 Arxiv-math 语料库以及长度超过 4K 的 C4 文本（Raffel 等人，2020 年）。在整个实验过程中，语段长度 N 设置为 2K。</p>
<h3 id="1M-passkey-检索基准"><a href="#1M-passkey-检索基准" class="headerlink" title="1M passkey 检索基准"></a>1M passkey 检索基准</h3><p>用 Infini-attention 替换 1B LLM 中的 Vanilla MHA，并继续对长度为 4K 的输入进行预训练。在对 passkey 检索任务进行微调之前，对模型进行了 30K 步的训练，batch size 为 64。</p>
<p>passkey 任务将一个随机数隐藏在长文本中，并在模型输出端进行回问。分心文本的长度可通过多次重复文本块来改变。之前的工作（Chen 等人，2023a）表明，8B LLaMA 模型在使用位置插值对 32K 长度的输入进行微调时，可以解决 32K 长度的任务。作者进一步挑战这一难题，仅对 5K 长度的输入进行微调，以测试 1M 长度的机制。</p>
<p><img src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Leave%20No%20Context%20Behind%20Efficient%20Infinite%20Context%20Transformers%20with%20Infini-attention/Table%203.png"></p>
<blockquote>
<p>表 3：当对 5K 长度的输入进行微调时，Infini-Transformer 解决了高达 1M 上下文长度的密码任务。报告了隐藏在长度为 32K 到 1M 的长输入的不同部分（开始&#x2F;中间&#x2F;结束）中的 token 级检索准确率。</p>
</blockquote>
<p>表 3 报告了输入长度从 32K 到 1M 不等的测试子集的 token 级准确率。对于每个测试子集，都控制了 passkey 的位置，使其位于输入序列的开头、中间或结尾。报告了 zero-shot 精度和微调精度。在对 5K 长度的输入进行 400 步微调后，Infini-Transformers 解决了高达 1M 上下文长度的任务。</p>
<h3 id="500K-长度的书籍摘要（BookSum）"><a href="#500K-长度的书籍摘要（BookSum）" class="headerlink" title="500K 长度的书籍摘要（BookSum）"></a>500K 长度的书籍摘要（BookSum）</h3><p>进一步扩展该方法，用 8K 输入长度的 8B LLM 模型持续预训练 30K 步。然后，对书籍摘要任务 BookSum 进行了微调，其目标是生成整本书的文本摘要。</p>
<p>将输入长度设置为 32K 以进行微调，并增加到 500K 以进行评估。使用 0.5 的生成温度和 top_p &#x3D; 0.95，并将解码长度设为 1024，以生成每本书的摘要。</p>
<p><img src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Leave%20No%20Context%20Behind%20Efficient%20Infinite%20Context%20Transformers%20with%20Infini-attention/Table%204.png"></p>
<blockquote>
<p>表 4：500K 长书籍摘要（BookSum）结果。BART、PRIMERA 和 Unlimiformer 结果来自 Bertsch 等人（2024 年）。</p>
</blockquote>
<p>表 4 将作者的模型与专为摘要任务构建的编码器-解码器模型（Lewis 等人，2019 年；肖等人，2021 年）及其基于检索的长文本扩展（Bertsch 等人，2024 年）进行了比较。作者的模型超越了之前的最佳结果，并通过处理书中的全部文本在 BookSum 上实现了新的 SOTA。</p>
<p><img src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Leave%20No%20Context%20Behind%20Efficient%20Infinite%20Context%20Transformers%20with%20Infini-attention/Figure%204.png"></p>
<blockquote>
<p>图 4：输入的书籍文本越多，Infini-Transformer 获得的 Rouge 总分越高。</p>
</blockquote>
<p>还在图 4 中绘制了对 BookSum 数据的验证拆分的 Rouge 总体得分。有一个明显的趋势表明，随着输入的书籍文本越来越多，Infini-Transformers 的摘要性能指标也在不断提高。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>一个有效的记忆系统不仅对理解长上下文中的 LLMs 至关重要，而且对推理、规划、不断适应新知识、甚至学习如何学习都至关重要。该工作将<strong>压缩记忆模块与 vanilla 点积注意力层紧密结合。对注意力层进行的这一微妙而又关键的修改，使 LLMs 能够在内存和计算资源受限的情况下处理无限长的上下文</strong>。</p>
<p>研究表明，该方法可以自然地扩展到百万长度的输入序列，同时在长上下文语言建模基准和书籍摘要任务中表现优于基线方法。作者还证明了该方法具有良好的长度泛化能力。在最多 5K 序列长度的 passkey 实例上进行微调的 1B 模型解决了 100 万长度的问题。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://clvsit.github.io">clvsit</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://clvsit.github.io/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ALeave-No-Context-Behind-Efficient-Infinite-Context-Transformers-with-Infini-attention/">https://clvsit.github.io/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ALeave-No-Context-Behind-Efficient-Infinite-Context-Transformers-with-Infini-attention/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://clvsit.github.io" target="_blank">clvsit 个人博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/long-context/">long context</a><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></div><div class="post_share"><div class="social-share" data-image="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Leave%20No%20Context%20Behind%20Efficient%20Infinite%20Context%20Transformers%20with%20Infini-attention/Figure%202.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9A%E3%80%8ASequence-can-Secretly-Tell-You-What-to-Discard%E3%80%8B%EF%BC%8C%E5%87%8F%E5%B0%91%E6%8E%A8%E7%90%86%E9%98%B6%E6%AE%B5%E7%9A%84-kv-cache/" title="论文阅读：《Sequence can Secretly Tell You What to Discard》，减少推理阶段的 kv cache"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">论文阅读：《Sequence can Secretly Tell You What to Discard》，减少推理阶段的 kv cache</div></div></a></div><div class="next-post pull-right"><a href="/FlashAttention-V1-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" title="FlashAttention V1 学习笔记"><img class="cover" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/dl/Attention/FlashAttentionV1/tiling%E5%AE%9E%E7%8E%B0%E5%8A%A8%E6%80%81softmax%E6%9B%B4%E6%96%B0%E8%BF%87%E7%A8%8B.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">FlashAttention V1 学习笔记</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AScaling-Transformer-to-1M-tokens-and-beyond-with-RMT/" title="论文阅读：Scaling Transformer to 1M tokens and beyond with RMT"><img class="cover" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Scaling%20Transformer%20to%201M%20tokens%20and%20beyond%20with%20RMT/Figure%202.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-22</div><div class="title">论文阅读：Scaling Transformer to 1M tokens and beyond with RMT</div></div></a></div><div><a href="/Long-Context-%E8%B0%83%E7%A0%94/" title="Long Context 调研"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-10</div><div class="title">Long Context 调研</div></div></a></div><div><a href="/%E5%B1%82%E5%89%AA%E6%9E%9D%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%AB%81%E6%8E%A5%E7%9A%84%E2%80%9C%E5%8F%8C%E7%94%9F%E8%8A%B1%E2%80%9D/" title="论文阅读：The Unreasonable Ineffectiveness of the Deeper Layers 层剪枝与模型嫁接的“双生花”"><img class="cover" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/The%20Unreasonable%20Ineffectiveness%20of%20the%20Deeper%20Layers/Figure%201.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-10</div><div class="title">论文阅读：The Unreasonable Ineffectiveness of the Deeper Layers 层剪枝与模型嫁接的“双生花”</div></div></a></div><div><a href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AA-Survey-on-Data-Selection-for-LLM-Instruction-Tuning/" title="论文阅读：A Survey on Data Selection for LLM Instruction Tuning"><img class="cover" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/A%20Survey%20on%20Data%20Selection%20for%20LLM%20Instruction%20Tuning/Figure%201.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-08</div><div class="title">论文阅读：A Survey on Data Selection for LLM Instruction Tuning</div></div></a></div><div><a href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AAlbert%EF%BC%9AA-Lite-Bert-for-self-supervised-learning-of-language-representations/" title="论文阅读：Albert：A Lite Bert for self-supervised learning of language representations"><img class="cover" src="https://pic1.zhimg.com/80/v2-8ea109ed6f3a3781951402a6cf7cc586_720w.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-28</div><div class="title">论文阅读：Albert：A Lite Bert for self-supervised learning of language representations</div></div></a></div><div><a href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ABag-of-Tricks-for-Efficient-Text-Classification/" title="论文阅读：Bag of Tricks for Efficient Text Classification"><img class="cover" src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Bag%20of%20Tricks%20for%20Efficient%20Text%20Classification/FIgure%201.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-01-22</div><div class="title">论文阅读：Bag of Tricks for Efficient Text Classification</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">clvsit</div><div class="author-info__description">人生不是戏剧，而我亦非主角</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">74</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">29</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">59</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/clvsit"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/clvsit" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">知乎和 CSDN 同名 clvsit，目前在将本地的笔记逐步迁移到这，所以会看到过去日期的文章不断增多</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-number">1.</span> <span class="toc-text">方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Infini-attention"><span class="toc-number">1.1.</span> <span class="toc-text">Infini-attention</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Scaled-Dot-product-Attention"><span class="toc-number">1.1.1.</span> <span class="toc-text">Scaled Dot-product Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%8B%E7%BC%A9%E8%AE%B0%E5%BF%86"><span class="toc-number">1.1.2.</span> <span class="toc-text">压缩记忆</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E5%92%8C%E6%9C%89%E6%95%88%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AA%97%E5%8F%A3"><span class="toc-number">1.2.</span> <span class="toc-text">内存和有效上下文窗口</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">2.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AF%AD%E8%A8%80%E5%BB%BA%E6%A8%A1"><span class="toc-number">2.1.</span> <span class="toc-text">长上下文语言建模</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LLM-%E6%8C%81%E7%BB%AD%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-number">2.2.</span> <span class="toc-text">LLM 持续预训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1M-passkey-%E6%A3%80%E7%B4%A2%E5%9F%BA%E5%87%86"><span class="toc-number">2.2.1.</span> <span class="toc-text">1M passkey 检索基准</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#500K-%E9%95%BF%E5%BA%A6%E7%9A%84%E4%B9%A6%E7%B1%8D%E6%91%98%E8%A6%81%EF%BC%88BookSum%EF%BC%89"><span class="toc-number">2.2.2.</span> <span class="toc-text">500K 长度的书籍摘要（BookSum）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">3.</span> <span class="toc-text">总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/PyramidKV/" title="PyramidKV">PyramidKV</a><time datetime="2024-06-14T12:27:21.000Z" title="发表于 2024-06-14 20:27:21">2024-06-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/Long-Context-%E8%B0%83%E7%A0%94/" title="Long Context 调研">Long Context 调研</a><time datetime="2024-06-10T14:43:57.000Z" title="发表于 2024-06-10 22:43:57">2024-06-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/vocab-size-%E7%A0%94%E7%A9%B6/" title="vocab size 调研">vocab size 调研</a><time datetime="2024-05-31T15:47:07.000Z" title="发表于 2024-05-31 23:47:07">2024-05-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARCoT-Detecting-and-Rectifying-Factual-Inconsistency-in-Reasoning-by-Reversing-Chain-of-Thought/" title="论文阅读：RCoT Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought"><img src="https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/RCoT%EF%BC%9ADetecting%20and%20Rectifying%20Factual%20Inconsistency%20in%20Reasoning%20by%20Reversing%20Chain-of-Thought/Figure%204.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读：RCoT Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought"/></a><div class="content"><a class="title" href="/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ARCoT-Detecting-and-Rectifying-Factual-Inconsistency-in-Reasoning-by-Reversing-Chain-of-Thought/" title="论文阅读：RCoT Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought">论文阅读：RCoT Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought</a><time datetime="2024-05-17T13:37:04.000Z" title="发表于 2024-05-17 21:37:04">2024-05-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/Transformer-%E6%9E%B6%E6%9E%84%E4%B8%AD%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/" title="Transformer 架构中的位置编码">Transformer 架构中的位置编码</a><time datetime="2024-05-16T06:56:59.000Z" title="发表于 2024-05-16 14:56:59">2024-05-16</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://markdown-picture-clvsit.oss-cn-hangzhou.aliyuncs.com/nlp/paper/Leave%20No%20Context%20Behind%20Efficient%20Infinite%20Context%20Transformers%20with%20Infini-attention/Figure%202.png')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By clvsit</div><div class="footer_custom_text">城南芳草城北溪，池塘烟柳归鸟栖。忽听窗外风吹雨，一梦花落月近西。</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"]):not([href="/music/"]):not([href="/no-pjax/"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener
  btf.removeGlobalFnEvent('pjax')
  btf.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>